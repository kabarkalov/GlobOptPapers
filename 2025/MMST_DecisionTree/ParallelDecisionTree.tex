% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%

\usepackage[utf8]{inputenc} % кодировка
\usepackage[english, russian]{babel} % Русские и английские переносы
\usepackage{cite}              % для корректного оформления литературы
\usepackage{enumitem}
\usepackage{amsmath} 


\begin{document}
%
\title{Параллельный запуск локальных методов алгоритме глобального поиска с использованием деревьев решений}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{К.А. Баркалов\inst{1}\orcidID{0000-1111-2222-3333} \and
И.Г. Лебедев\inst{2,3}\orcidID{1111-2222-3333-4444} \and
Я.В. Селенко\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Нижегородский государственный университет им. Н.И. Лобачевского, Нижний Новгород, Российская Федерация }
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
В данной работе предлагается параллельный подход к запуску локальных методов при решении задачи глобальной оптимизации алгоритмом глобального поиска. Целевая функция задачи задана как черный ящики и от нее требуется лишь удовлетворять условию Липшица с априори неизвестной константой. В работе рассматривается метод выбора окрестности локальных экстремумов целевой функции на основе анализа накопленной поисковой информации. Проведение такого анализа с использованием методов машинного обучения позволяет принять решение о параллельном запуске локальных методов, что может ускорить сходимость алгоритма. Это предположение было подтверждено результатами численных экспериментов, демонстрирующих ускорение при решении ряда тестовых задач.

\keywords{
	Глобальная оптимизация $\cdot$ 
	Многоэкстремальные функции $\cdot$ 
	Параллельные вычисления $\cdot$ 
	Машинное обучение $\cdot$ 
	Древо решений $\cdot$}
\end{abstract}
%
%
%



\section{Введение}

В настоящей работе рассматриваются параллельные алгоритмы решения многомерных задач глобальной оптимизации. Такие задачи часто возникают в тех случаях, когда необходимо подобрать значения параметров исследуемой математической модели, при которых результаты моделирования лучше всего соответствуют экспериментальным данным. Для решения задач этого класса известно множество алгоритмов: от метаэвристических, основанных на идее случайного поиска \cite{Ferreiro2013,Garcia2014,Langdon2011}, до детерминированных алгоритмов, гарантирующих сходимость к глобальному минимуму \cite{Evtushenko2009,He2008,Paulavicius2011}.

Поскольку в реальных задачах глобальной оптимизации каждое вычисление значения функции (далее \textit{испытание}) является операцией, требующей больших вычислительных затрат, количество таких попыток приходится сокращать. Это может быть достигнуто за счет преднамеренного выбора вариантов в ходе поиска оптимального решения, отсекая бесперспективные поисковые под области и исследуя только те, в которых можно найти решение задачи. Алгоритм глобального поиска (АГП) основан на этой идее \cite{Strongin2000}. В настоящей работе мы попытались объединить АГП и метод локальной оптимизации \cite{Nocedal, Kelley}  (метод поиска по образцу Хука-Дживса \cite{HookJeeves}), чтобы уменьшить количество выполняемых испытаний. Решение о запуске локального метода будет приниматься с использованием дерева решений.

\section{Алгоритмы}\label{SecA}

Рассмотрим задачу поиска глобального минимума функции $\varphi(y)$ на гиперинтервале $D=\{ y\in\ R^N:\ a_i\le\ y_i\le\ b_i,\ 1\le\ i\le\ n \}$. 
\begin{equation} \label{sec:problem}   
	\varphi(y^*) = min\{\varphi(y):y\in D\}, D = \{y \in R^N : a_i \leq y_i \leq b_i, 1 \leq i \leq N \},
\end{equation}
где $a,b \in R$ — заданные векторы.

Также предположим, что функция удовлетворяет условию Липшица с \textit{априори} неизвестной константой $L$, что соответствует ограниченному изменению значений функции при ограниченном изменении аргумента.
\begin{displaymath} 
	|\varphi(y_1)-\varphi(y_2)|\leq L\parallel y_1-y_2 \parallel ,y_1,y_2 \in D, 0<L< \infty. 
\end{displaymath}
Это предположение можно рассматривать (применительно к реальным задачам) как отражение ограниченной мощности, порождающей изменения в моделируемой системе.

Численное решение задачи (\ref{sec:problem}) сводится к построению оценки $y_k^\ast\in D$, соответствующей некоторому понятию близости к точке $y^\ast$ (например, ${ ||y^\ast-y}_k^\ast||\le\ \varepsilon$, где $\varepsilon\geq0$ — заданная точность) на основе конечного числа $k$ вычисленных значений целевой функции. Применительно к классу рассматриваемых задач предполагается, что целевая функция $\varphi(y)$ может быть определена алгоритмически, в результате выполнения некоторой подпрограммы или библиотеки.

При решении многомерных задач применяется уменьшение размерности (т.е. сведение многомерной задачи к эквивалентной одномерной) с помощью кривых Пеано. Они позволяют свести многомерную задачу оптимизации в области $D$ к одномерной задаче минимизации в интервале $[0, 1]$.
\begin{displaymath}
	\varphi(y(x^\ast))\ =\min\{\varphi(y(x)): x\in [0,1]\},
\end{displaymath}
где функция $\varphi(y(x^\ast))$ удовлетворяет более общему условию Гельдера
\begin{displaymath}
	\left|\varphi (y \left(x_1\right))- \varphi (y \left(x_2\right)\right )|\le\ H\left|x_1-x_2\right|^\frac{1}{N},\ x_1,\ x_2\epsilon[0,1].
\end{displaymath} 

Поэтому вместо исходной задачи минимизации функции $\varphi(y)$ в области $D$ можно рассмотреть минимизацию одномерной функции $f(x)=\varphi(y(x)) $, удовлетворяющий условию Гельдера для $ x\in [0,1]$.


Чтобы определить, в какой момент лучше запускать локальный метод в рамках глобального поиска, воспользуемся алгоритмом, основанным на дереве решений. На первом этапе поиска происходит накопление поисковой информации в виде результатов выполненных испытаний. На основе результатов испытаний проводилось обучение дерева решений, позволяющего получать кусочно-постоянную аппроксимацию целевой функции и на ее основе прогнозировать поведение целевой функции. 

Дерево решений — это инструмент для автоматизированного анализа больших массивов данных, применяемый в машинном обучении. Его можно использовать как для решения задач классификации, так и для регрессии. В случае построения регрессии каждому листу дерева присваивается константа. Поэтому полученная аппроксимирующая функция является кусочно-постоянной. В нашем случае дерево решений строит функцию $\psi(y)$ как кусочно-постоянную аппроксимацию $\varphi(y)$ в области поиска. Обозначим значение, вычисленное деревом решений в точке $y$, как $z' = \psi(y)$.
На рис. \ref{fig:fig2} представлен график целевой функции $\varphi(y)$; На рис. \ref{fig:fig2_2} представлена соответствующая аппроксимация $\psi(y)$, построенная с помощью дерева решений.


\begin{figure}
	\begin{center}
		\begin{minipage}[h]{0.7\linewidth}
			\includegraphics[width=1\linewidth]{figure/fig5.png}
			\caption{Целевая функция $\varphi(y)$ из тестового класса} %% подпись к рисунку
			\label{fig:fig2}
		\end{minipage}
	\end{center}
\end{figure}	

\begin{figure}
	\begin{center}
		\begin{minipage}[h]{0.7\linewidth}
			\includegraphics[width=1\linewidth]{figure/fig4.png}
			\caption{Кусочно-постоянная аппроксимация $\psi(y)$ функции $\varphi(y)$} %% подпись к рисунку
			\label{fig:fig2_2}
		\end{minipage}
	\end{center}
\end{figure}	


При реализации алгоритма поиска областей притяжения локальных минимумов для построения дерева решений использовались алгоритмы из библиотеки OpenCV. OpenCV — это библиотека с открытым исходным кодом алгоритмов компьютерного зрения, обработки изображений и численных алгоритмов общего назначения. Более подробную информацию о дереве решений можно найти в \cite{Brahmbhatt2013}.




После определения с помощью дерева решений области притяжения локального минимума, из ближайшей точки запускаем локальный метод. Алгоритм поиска локального минимума Хука--Дживса относится к классу методов нулевого порядка. Его правила вычисления представляют собой комбинацию исследовательского поиска (для выбора направления) и поиска в выбранном направлении \cite{Himmelblau72}.

\begin{figure} 
	\begin{center} 
		\begin{minipage}[h]{0.8\linewidth} 
			\includegraphics[width=1\linewidth]{figure/fig1.pdf} 
			\caption{Пример итераций алгоритма Хука-Дживса} %%  подпись к рисунку 
			\label{fig:fig1} 
		\end{minipage} 
	\end{center} 
\end{figure}	


На рис. \ref{fig:fig1} показан пример работы алгоритма. Отображаются линии  уровня, закрашенные кружки обозначают успешные шаги, незакрашенные кружки соответствуют неудачным шагам, выполненным во время исследовательского поиска. 



\section{Многомерный параллельный алгоритм глобального поиска}\label{SecGSA}

На начальном этапе мастер-процесс (пусть он имеет идентификатор $0$) параллельно запускает $p$ испытаний в $p$ разных точках $\{y\left(x^1\right),y\left(x^2\right),\ldots,y\left(x^p\right)\}$ отрезка $[0,1]$.
Прообразами двух точек являются граничные $x^1=0,x^p=1$; остальные — внутренние точки $x^i\in\left(0,1\right),i=2,\ldots,p-1$.

Теперь предположим, что алгоритм уже сделал $K\geq0$ итераций и проведено $k$ испытаний.


\begin{enumerate}
	
	\item Извлекаем из множество $U$ (координат точек испытания) $c = max(p, |U|)$ координат точек испытания $y^{k+1}=y\left(x^{k+1}\right) ... y^{k+c}=y\left(x^{k+c}\right)$. Если $c < p$ то сначала вычисляем $n = p - c$ точек согласно шагам 2 - 7, иначе сразу переходим к шагу 8.	
	
	\item Перенумеруем (по нижними индексами) точки $x^i, 1\leq i\leq k$, а также граничные точки отрезка $[0,1]$ в порядке возрастания координаты  
	\begin{equation} 
		\label{agp1_sort} 	0=x_0<\ x_1<\ ...\ <x_{k+1}=1. 	
	\end{equation} 
	и сопоставить их со значениями $z_i=f(x_i)$. 
	
	\item  Вычислить текущую нижнюю оценку $M$ неизвестной константы Гельдера $H$: 
	\begin{equation} 
		\label{agp2_mu} 	\mu=max\left\{\frac{|z_i-z_{i-1}|}{{{(x}_i-x_{i-1})}^{1/N}},\ i=1,\ldots,k\right\},\ M=\  \left\{\begin{matrix}r\mu,\ \mu>0,\\1,\ \mu=0,\\\end{matrix}\right.\ 	
	\end{equation} 
	где $r>1$ — параметр алгоритма. Этот параметр управляет надежностью алгоритма: более высокие значения $r$ гарантируют гарантированное нахождение глобального минимума, выбор меньшего значения ускоряет сходимость алгоритма.
	
	\item  Для каждого интервала $(x_{i-1},x_i), 1\leq i\leq k+1,$ вычислить значение $R(i)$, называемое \textit{характеристикой} интервала, по формулам
	
	\begin{equation} 
		\label{agp3_R1} R(1)=2\Delta_1-4\dfrac{z_1}{M}, \; R(k+1)=2\Delta_{k+1}-4\dfrac{z_k}{M}, 
	\end{equation} 
	
	\begin{equation} 
		\label{agp3_Ri} R(i)=\Delta_i+\dfrac{(z_i-z_{i-1})^2}{M^2\Delta_i}-2\dfrac{z_i+z_{i-1}}{M},1<i<k+1, 
	\end{equation} 
	
	где \(\Delta_i=(x_i-x_{i-1})^\frac{1}{N}\).
	
	\item   Упорядочить характеристики $R\left(i\right),\ 1\leq i \leq k+1,$ в порядке не возрастания
	\begin{equation} 
		\label{agp4_R_sort} 	R\left(t_1\right)\geq\ R\left(t_2\right)\geq...\geq\ R\left(t_k\right)\geq\ R(t_{k+1}),\  
	\end{equation} 
	и выделить $n$ интервалов с индексами $t_j,\ 1\le\ j\le\ n$  с наибольшими значениями характеристик.	
	
	\item Вычислить новые координаты точки испытания $y'^{k+j}=y\left(x^{k+j}\right), \ 1\leq j\leq n$ прообразом которого является $x^{k+j}\in\left(x_{t_j-1},x_{t_j}\right)$, в соответствии с формулой
	\begin{equation}
		\label{agp5_x1}
		x^{k+1}=\frac{x_t+x_{t-1}}{2}-\mathrm{sign}\left(z_t-z_{t-1}\right)\frac{1}{2r}\left[\frac{\left|z_t-z_{t-1}\right|}{M}\right]^N.
	\end{equation}	
	
	
	\item Отправить на процесс $j, \ 1\leq j\leq n$ координату $y'^{k+1} ... y'^{k+n}$  и сообщаем процессу, что нужно произвести одиночное испытание.
	
	\item Отправить на процессы $h, \ 1\leq h\leq c$ координаты точек $y^{k+1} ... y^{k+c}$ и сообщаем процессу, что нужно запустить локальный метод.
	
	\item Получить со всех $p$ процессов значения испытаний и добавить их в поисковую информацию.
	
	\item Добавить точки испытаний $y'^{k+1}=y\left(x^{k+1}\right) ... y'^{k+n}=y\left(x^{k+т}\right)$ к множеству $V$.
	
	\item Если $k\ <\ 100 N$, перейти к шагу 1.
	
	
	\item Строим дерево решений по всей накопленной поисковой информации, получаем аппроксимирующую функцию $\psi(y)$.
	
	\item Если дерево решений используется впервые, постройте равномерную сетку
	\begin{displaymath} 
		Y'=\{ y'\in R^N:\ a_i\le y'_i \le b_i,\ 1\le i\le N \},
	\end{displaymath} 
	где количество узлов сетки является параметром метода.
	
	\item Вычислить кусочно постоянную аппроксимацию функции $\psi(y)$, по дереву решений : $Z' = \{ z'=  \psi(y'), y' \in Y'\}$
	
	\item Для всех точек $y'\in V$ найдите точки $y'_q$, ближайшие к $y'$,
	
	Если ни одна точка $y'_q$ не имеет меньшего значения, чем $y'_q$, поместить $y'$ в множество $U$
	
	Очистить множество $V$.
	
	\item Проверка критериев остановки
	
	
\end{enumerate}

Следующие $p$ испытания выполняются параллельно в точках $x^{k+j},\ 1\leq j\leq p$, вычисляемых по формуле (\ref{agp5_x1}). По завершении испытаний результаты этих испытаний сохраняются в базе поисковой информации, а алгоритм переходит к вычислению новых точек испытания.
Отметим, что, как правило, процесс испытания в прикладных оптимизационных задачах гораздо более затратный по вычислительным ресурсам по сравнению с вычислением координаты точки.

Алгоритм останавливается в том случае, если выполняется условие \(\Delta_{t_j} < \varepsilon\) хотя бы для одного значения $t_j,\ 1\le\ j\le\ p$, из (\ref{agp4_R_sort} ). Этот критерий остановки (наряду с обычным для итерационных методов критерием ограничения числа выполняемых итераций) используется в прикладных задачах оптимизации, в которых точка глобального минимума $y^*$ априори неизвестна.

При решении тестовых задач, в которых известна точка глобального минимума $y^*$, можно использовать критерий остановки и по попаданию в окрестность глобального минимума. В этом случае метод останавливается, если выполняется условие $\left\|y(x_{t_j})-\ y^\ast\right\| < \varepsilon$ для одного $t_j,\ 1\le\ j\le\ p$ из (\ref{agp4_R_sort}).

В качестве окончательной оценки глобального минимума рассматриваемой задачи берется значение 
\begin{equation} 
	f_k^*=\min_{1\leq i \leq k}f(x_i), \; x_k^*=arg \min_{1\leq i \leq k}f(x_i). 
\end{equation} 


Обоснование такого способа организации вычислений см. в \cite{Strongin2000,Barkalov2016}. Модификации с учетом наличия ограничений-неравенств, а также информация о производной целевой функции представлены в \cite{Barkalov2002,Gergel1997}.





% переводить до этого раздела
\section{Численные эксперименты}\label{SecR}


Численные эксперименты проводились на суперкомпьютере Лобачевского. Каждый узел имел по два процессора Intel Sandy Bridge E5-2660 2,2 ГГц, 64 Гб оперативной памяти.

В экспериментах использовался генератор тестовых задач GKLS, который может генерировать задачи многоэкстремальной оптимизации с известными свойствами: точка глобального минимума, количество локальных минимумов и др.


Ниже представлены результаты работы синхронного параллельного алгоритма глобального поиска  с использованием дерева решений для нахождения областей притяжения локальных минимумов. Численное сравнение проводилось на двух классах функций GKLS (Simple и Hard из \cite{Sergeyev2006}) размерностей 2, 3, 4 и 5.
Эти два класса отличаются размером области притяжения точки глобального минимума.
Для простого класса задач радиус области притяжения в три раза больше, чем для сложного.

Критерием остановки алгоритмов было попадание очередных пробных точек в $\varepsilon$-близость к истинному глобальному минимуму. В таблицах \ref{tab:1} \ref{tab:2} сравниваются усредненные числа итераций, выполняемых алгоритмами при решении задач, и значения ускорения времени относительно последовательного прогона. Распараллеливание выполнено по технологии MPI, эксперимент выполнен на 8, 16, 32 и 64 процессах.


\begin{table}[!ht]
	\caption{Ускорение, выполненных разными алгоритмами}
	\label{tab:2}
	\centering	
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		2 & 8 mpi & 16 mpi & 32 mpi & 64 mpi  \\ \hline
		hard & 6,416962112 & 7,429653192 & 16,44977408 & 7,698208433  \\ \hline
		simple & 11,19015498 & 5,017982941 & 13,58978911 & 30,41682225  \\ \hline
		3 & ~ & ~ & ~ &   \\ \hline
		hard & 4,010618178 & 6,760486061 & 22,40788476 & 23,48393504  \\ \hline
		simple & 3,975141387 & 5,069355695 & 14,97196995 & 14,48568142  \\ \hline
		4 & ~ & ~ & ~ &   \\ \hline
		hard & 3,516700024 & 7,767619059 & 15,63797476 & 22,60877765  \\ \hline
		simple & 4,204314065 & 6,771626114 & 7,265190222 & 5,789337963  \\ \hline
		5 & ~ & ~ & ~ &   \\ \hline
		hard & 10,28406793 & 18,45856679 & 38,75628766 & 30,57030882  \\ \hline
		simple & 10,79552171 & 15,93214599 & 31,96240356 & 31,65420434  \\ \hline
	\end{tabular}
\end{table}




Как видно из таблицы \ref{tab:2}, ускорение итерации достаточно велико для задач любой размерности.





\section{Conclusion} 

На данный момент в результате проделанной работы удалось совместить алгоритм глобального поиска с методом локальной оптимизации. В отличие от известных многозаходных схем решение о запуске локального метода принимается с помощью дерева решений. Использование такой комбинации методов позволяет значительно ускорить работу алгоритма.

Параллельная версия алгоритма сохраняет свойства своего последовательного прототипа, что было подтверждено численными экспериментами по решению серии из нескольких сотен задач различной размерности. Предлагаемая схема позволяет использовать преимущества как распараллеливания, так и быстрого поиска локальных экстремумов.

Помимо использования деревьев решений для выявления областей притяжения локальных экстремумов многоэкстремальных функций, мы также планируем использовать методы машинного обучения для разделения переменных решаемой задачи. Во многих прикладных задачах оптимизации зависимость целевой функции от некоторых параметров бывает либо линейной, либо одномодальной. Выделение таких переменных в специальную группу и решение задачи с помощью параллельной схемы рекурсивной оптимизации \cite{Barkalov2020_1} позволяет сократить время решения задачи на порядки по сравнению с использованием глобального поиска сразу по всем переменным.

%
% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\bibliography{bibliography}{}











\end{document}
