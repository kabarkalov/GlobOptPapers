% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%

%\usepackage[utf8]{inputenc} % кодировка
\usepackage[english]{babel} % Русские и английские переносы
\usepackage{cite}              % для корректного оформления литературы
\usepackage{enumitem}
\usepackage{amsmath} 


\begin{document}
%
\title{Parallel global optimization algorithm employing decision trees for launching local methods}
%
\titlerunning{Parallel global optimization algorithm employing decision trees}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Konstantin Barkalov$^{1,2}$\orcidID{0000-0001-5273-2471} \and
Ilya Lebedev$^{1,2}$\orcidID{0000-0002-8736-0652} \and
Dmitry Silenko$^{1,2}$\orcidID{0000-0002-2578-9699} }
%
\authorrunning{K. Barkalov et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{$^1$Lobachevsky State University of Nizhni Novgorod, Nizhni Novgorod, Russia\\$^2$Research and Education Mathematics Center ``Mathematics for Future Technologies'', Nizhni Novgorod, Russia}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
%В данной работе разработан подход к параллельному запуску локальных методов при решении задачи глобальной оптимизации с помощью алгоритма глобального поиска. Целевая функция задачи задана как черный ящик; предполагается, что она удовлетворяет условию Липшица с априори неизвестной константой. В работе рассматривается метод выбора окрестности локальных экстремумов целевой функции на основе анализа накопленной поисковой информации. Проведение такого анализа с использованием методов машинного обучения позволяет принять решение о параллельном запуске локальных методов, что позволяет ускорить сходимость алгоритма. Это предположение было подтверждено результатами численных экспериментов, демонстрирующих ускорение при решении серии тестовых задач.

This paper presents an approach for the parallel launch of local optimization methods within a global search algorithm framework, aimed at solving global optimization problems. The objective function is treated as a black-box and is assumed that it satisfies the Lipschitz condition with an unknown constant. The proposed method focuses on selecting the region of attraction of local extrema of the objective function based on the analysis of accumulated search data. Employing machine learning techniques for this analysis enables informed decisions regarding the parallel execution of local methods, leading to improved algorithm convergence. The effectiveness of this approach is validated through numerical experiments, which demonstrate accelerated performance on a set of benchmark problems.


\keywords{
Global optimization $\cdot$
Multi-extremal functions $\cdot$
Parallel computing $\cdot$
Machine learning $\cdot$
Decision tree $\cdot$}
\end{abstract}
%
%
%



\section{Introduction}

Many decision-making problems arising in various areas of human activity can be formulated as optimization problems. With the rapidly increasing complexity of optimized objects, it is quite natural for their mathematical models to become more complex as well, which consequently complicates the search for an optimal combination of parameters. Very often, it is impossible to find such a combination analytically, making it necessary to construct numerical methods for its search.

%В настоящей работе рассматриваются параллельные алгоритмы решения многомерных задач глобальной оптимизации. Такие задачи часто возникают в тех случаях, когда необходимо подобрать значения параметров исследуемой математической модели, при которых результаты моделирования лучше всего соответствуют экспериментальным данным. Для решения задач этого класса известно множество алгоритмов: от метаэвристических, основанных на идее случайного поиска \cite{Ferreiro2013,Garcia2014,Langdon2011}, до детерминированных алгоритмов, гарантирующих сходимость к глобальному минимуму \cite{Evtushenko2009,He2008,Paulavicius2011}.
This paper investigates parallel algorithms for solving multidimensional global optimization problems. These problems commonly occur when optimizing the parameters of a mathematical model to achieve the best possible agreement between simulation results and experimental observations. Numerous algorithms have been developed to address this class of problems, varying from metaheuristic and evolutionary techniques \cite{Ferreiro2013,Garcia2014,Langdon2011} to deterministic algorithms designed to guarantee convergence to a global minimizer \cite{Evtushenko2009,He2008,Paulavicius2011}.


%Поскольку в реальных задачах глобальной оптимизации каждое вычисление значения функции (далее \textit{trial}) является операцией, требующей больших вычислительных затрат, количество таких попыток приходится сокращать. Это может быть достигнуто за счет преднамеренного выбора вариантов в ходе поиска оптимального решения, отсекая бесперспективные поисковые под области и исследуя только те, в которых можно найти решение задачи. Алгоритм глобального поиска (АГП) основан на этой идее \cite{Strongin2000}. В настоящей работе мы попытались объединить АГП и метод локальной оптимизации \cite{Nocedal, Kelley}  (метод поиска по образцу Хука-Дживса \cite{HookJeeves}), чтобы уменьшить количество выполняемых испытаний. Решение о запуске локального метода будет приниматься с использованием дерева решений.
In real-world global optimization problems, each function evaluation (hereafter referred to as a \textit{trial}) is a computationally expensive operation, making it necessary to reduce the number of such trials. This may be achieved by the purposeful selection of alternatives during the course of optimization, rejecting unpromising search sub-regions and exploring only those where a solution can be found. 
%именно на такой идее основан один из эффективных детерминированных методов глобальной оптимизации - Алгоритм Глобального поиска
This idea forms the foundation of an efficient deterministic global optimization method: the Global Search Algorithm (GSA) \cite{Strongin2000}.
In this work, we attempted to combine the GSA and a local optimization method (the Hooke-Jeeves method \cite{Nocedal, Kelley}) to decrease the number of trials carried out.
A decision tree will determine whether to run the local method.

% Следует отметить, что и другие подходы к решению задач глобальной оптмизации используют схожие идеи, основанные на использования локального уточнения лучшего найденного решения для ускорения сходимости к точке глобального оптимума. Сказанное справедливо как для эволюционных, так и для детерминированных алгоритмов глобальной оптмизации. 

It should be noted that other approaches to solving global optimization problems use similar ideas based on employing local refinement of the best-found solution to accelerate convergence to the global optimizer. This holds true for both evolutionary \cite{Kuzenkov2024} and deterministic \cite{Grishagin2025} global optimization algorithms.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%Классические алгоритмы локальной оптимизации предназначены для нахождения единственного экстремума (локального минимума или максимума) в окрестности начального приближения. В зависимости от используемой информации о целевой функции, эти методы принято классифицировать на:

Classical local optimization algorithms are designed to find a single extremum (local minimum or maximum) in the neighborhood of an initial guess. Depending on the information used about the objective function, these methods are typically classified as:
\begin{itemize}
	\item Zero-order methods, which use only function values;
	\item First-order methods (gradient methods), requiring the computation of first derivatives;
	\item Second-order methods (e.g., Newton's method), which require the computation of the matrix of second derivatives (Hessian).
\end{itemize}


%Важно подчеркнуть, что применение градиентных методов (первого и, в особенности, второго порядка) для оптимизации функций типа черного ящика сопряжено с фундаментальными трудностями. Основная проблема заключается в отсутствии аналитического описания функции, что делает невозможным прямое вычисление производных и приводит к необходимости их численной оценки, что, в свою очередь, может быть вычислительно затратно и вносить дополнительные погрешности \cite{Kelley}. В связи с этим, в рамках данного исследования дальнейшее рассмотрение будет ограничено исключительно методами нулевого порядка, не требующими вычисления производных.
It is important to emphasize that the application of gradient methods (first-order and, especially, second-order) to the optimization of black-box functions is associated with fundamental difficulties. The main problem lies in the absence of an analytical description of the function, which makes direct computation of derivatives impossible and leads to the need for their numerical estimation. This, in turn, can be computationally expensive and introduce additional errors \cite{Kelley}. In light of this, the remainder of this study will be limited exclusively to zero-order methods, which do not require the computation of derivatives.


%Ключевой вопрос при гибридизации глобального и локального поиска заключается в определении оптимального момента для запуска локального метода. Для решения этой задачи предлагается использовать адаптивный алгоритм, основанный на машином обучении, а именно на построении дерева решений.
%На первоначальном этапе глобального поиска информация о поведении целевой функции накапливается в виде набора вычисленных значений в точках испытаний. Эти данные используются для обучения модели дерева решений. Обученная модель строит кусочно-постоянную аппроксимацию целевой функции, которая позволяет прогнозировать её поведение в ещё не исследованных областях.
%Механизм принятия решения о запуске локального поиска заключается в следующем: для точки, подозрительной на наличие в её окрестности локального минимума, с помощью аппроксимирующей модели анализируются значения в соседних точках. На основе этого анализа делается вывод о том, попадает ли следующая точка испытания глобального алгоритма в область притяжения данного локального минимума. Это позволяет своевременно запустить локальную оптимизацию для эффективного уточнения решения в данной области.

A key question when hybridizing global and local search is determining the optimal moment to initiate the local method. To address this, we propose using an adaptive algorithm based on machine learning, specifically on building a decision tree.

In the initial phase of the global search, information about the behavior of the objective function is accumulated in the form of a set of computed values at trial points. This data is used to train a decision tree model. The trained model constructs a piecewise-constant approximation of the objective function, which allows predicting its behavior in unexplored regions.

The mechanism for deciding when to initiate local search is as follows: for a point suspected of having a local minimum in its vicinity, the approximating model is used to analyze values in neighboring points. Based on this analysis, a conclusion is drawn as to whether the next trial point of the global algorithm falls within the basin of attraction of this local minimum. This allows timely initiation of local optimization for effective refinement of the solution in that region.

The content of this paper is structured in the following manner. 
Section \ref{SecA} addresses the methodological basis of the research. It outlines the main idea of the global search algorithm used and details the computational rules of its operation. Furthermore, the section includes a description of the employed local optimization method (zero-order) and justifies the technique for constructing a function approximation using decision trees.
Section \ref{SecGSA} presents the key contribution of this work: a novel hybrid algorithm integrating global search with local refinement, managed via predictions from a decision tree model.
Section \ref{SecR} contains the results of computational experiments conducted on a parallel computing system, their analysis and interpretation, and the formulation of the main concluding remarks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem statement and solution strategies}\label{SecA}

The problem under consideration is to find the global minimum point of the function $\varphi(y)$ over a hyperinterval $D$, 
\begin{equation} \label{sec:problem}   
	\varphi(y^*) = \min\{\varphi(y):y\in D\}, \; D = \{y \in R^N : a_i \leq y_i \leq b_i, 1 \leq i \leq N \},
\end{equation}
where $a,b \in R$ are given vectors.

We assume that the function satisfies the Lipschitz condition 
\begin{displaymath} 
	|\varphi(y_1)-\varphi(y_2)|\leq L \left\| y_1-y_2 \right\|, \; y_1,y_2 \in D, 
\end{displaymath}
with a constant $L, \; 0<L< \infty$,  that is unknown a priori.
This condition corresponds to a bounded change in the function's values for a bounded change in the argument.
The assumption that the objective function satisfies the Lipschitz condition can be considered (in the context of applied problems) as a reflection of the bounded possible variations in the system under optimization.

Numerically solving the problem (\ref{sec:problem}) entails constructing an estimate $y^k\in D$ aligns with a specific concept of closeness to the point $y^\ast$ (e.g., $\left\|y^\ast-y^k \right\| \le\ \delta$, where $\delta\geq0$ is a given accuracy) based on a finite number $k$ of computed function values. With respect to the class of problems considered, we assume that the function $\varphi(y)$ can be algorithmically defined through the execution of a subroutine.

The approach under consideration employs a dimensionality reduction scheme using Peano curves to solve multidimensional problems.
These curves enable the conversion of a multidimensional optimization problem within the region $D$ into a one-dimensional minimization problem over an interval $[0,1]$,
\begin{displaymath}
	\varphi(y(x^\ast))\ =\min\{\varphi(y(x)): x\in [0,1]\},
\end{displaymath}
where function $\varphi(y(x^\ast))$ satisfies the Hölder condition
\begin{displaymath}
	\left|\varphi (y \left(x_1\right))- \varphi (y \left(x_2\right)\right )|\le\ H\left|x_1-x_2\right|^\frac{1}{N},\ x_1, x_2\in[0,1].
\end{displaymath} 

Consequently, rather than addressing the original problem of minimizing the function $\varphi(y)$ within the region $D$, we can consider minimizing the one-dimensional function $f(x)=\varphi(y(x))$ for $x \in [0, 1]$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This approach is discussed in the works \cite{Sergeyev2022, Usova2024}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%Чтобы определить, в какой момент лучше запускать локальный метод в рамках глобального поиска, воспользуемся алгоритмом, основанным на дереве решений. На первом этапе поиска происходит накопление поисковой информации в виде результатов выполненных испытаний. На основе результатов испытаний проводилось обучение дерева решений, позволяющего получать кусочно-постоянную аппроксимацию целевой функции и на ее основе прогнозировать поведение целевой функции. 
To identify when to best launch a local method  within the framework of a global search, we employ an algorithm based on a decision tree  \cite{Barkalov2023_2}. The first phase of the search involves accumulating search information in the form of results from performed trials.
Based on these trial results, a decision tree is trained to obtain a piecewise-constant approximation of the objective function for predicting its behavior.

%Дерево решений — это инструмент для автоматизированного анализа больших массивов данных, применяемый в машинном обучении. Его можно использовать как для решения задач классификации, так и для регрессии. В случае построения регрессии каждому листу дерева присваивается константа. Поэтому полученная аппроксимирующая функция является кусочно-постоянной. В нашем случае дерево решений строит функцию $\psi(y)$ как кусочно-постоянную аппроксимацию $\varphi(y)$ в области поиска. Обозначим значение, вычисленное деревом решений в точке $y$, как $z' = \psi(y)$.
%A decision tree is a tool for automated analysis of large datasets used in machine learning. It can be used for both classification and regression problems. In the case of regression, each leaf of the tree is assigned a constant. Therefore, the resulting approximating function is piecewise-constant. In our case, the decision tree constructs a function $\psi(y)$ as a piecewise-constant approximation of $\varphi(y)$ in the search space. Let $z' = \psi(y)$ denote the value computed by the decision tree at point $y$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Дерево решений представляет собой широко распространенный и мощный инструмент машинного обучения, предназначенный для автоматического выявления сложных зависимостей в больших многомерных массивах данных. В контексте нашего исследования данный метод применяется для построения вспомогательной аппроксимирующей модели с целью ускорения процесса глобальной оптимизации.

%С формальной точки зрения, используемая модель представляет собой бинарное дерево, то есть такую древовидную структуру, где каждый внутренний (нелистовой) узел имеет ровно два дочерних узла. Этот универсальный инструмент может применяться для решения двух типов задач: классификации и регрессии. В задачах классификации результатом обработки объекта является отнесение его к одному из классов, для чего каждый терминальный узел (лист) дерева помечается соответствующей меткой класса, при этом несколько листьев могут содержать идентичные метки. В задачах регрессии, что более релевантно для нашего случая, каждому листу ставится в соответствие некоторое постоянное (скалярное) значение. Таким образом, итоговая регрессионная модель представляет собой кусочно-постоянную функцию, принимающую на различных участках области определения фиксированные значения.

%В рамках предложенного подхода дерево решений используется для построения вспомогательной функции $\psi(y)$, которая служит кусочно-постоянной аппроксимацией некоторой заданной функции $\varphi(y)$ в пределах исследуемой области поиска. Значение, вычисляемое данной моделью в произвольной точке $y$, обозначается как $z' = \psi(y)$. Эта аппроксимация позволяет эффективно оценивать поведение целевой функции и принимать решения о дальнейшем направлении вычислительных ресурсов в процессе оптимизации.

A decision tree is a widespread and powerful machine learning tool designed to automatically identify complex dependencies in large multidimensional datasets. In the context of our research, this method is used to build an auxiliary approximating model in order to accelerate the global optimization process.

From a formal point of view, the model used is a binary tree, that is, a tree structure where each internal node has exactly two child nodes. This versatile tool can be used to solve two types of problems: classification and regression. In classification tasks, the result of processing an object is to assign it to one of the classes, for which each terminal node of the tree is marked with the corresponding class label, while several leaves may contain identical labels. In regression problems, which is more relevant for our case, each sheet is assigned a certain constant (scalar) value. Thus, the final regression model is a piecewise constant function that takes fixed values in different parts of the definition area.

In the framework of the proposed approach, a decision tree is used to construct an auxiliary function $\psi(y)$, which serves as a piecewise-constant approximation of a given function $\varphi(y)$ within the search area under study. The value calculated by this model at any point $y$ is denoted as $z' = \psi(y)$. This approximation makes it possible to effectively evaluate the behavior of the objective function and make decisions about the further direction of computing resources in the optimization process.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




Fig. \ref{fig:fig2} shows the graph of the function $\varphi(y)$; Fig. \ref{fig:fig2_2} illustrates its approximation $\psi(y)$ constructed using a decision tree.
\begin{figure}
	\begin{center}
		\begin{minipage}[h]{0.7\linewidth}
			\includegraphics[width=1\linewidth]{figure/fig5.png}
			\caption{Graph of the benchmark function $\varphi(y)$} %% подпись к рисунку
			\label{fig:fig2}
		\end{minipage}
	\end{center}
\end{figure}	

\begin{figure}
	\begin{center}
		\begin{minipage}[h]{0.7\linewidth}
			\includegraphics[width=1\linewidth]{figure/fig4.png}
			\caption{Graph of the piecewise-constant approximation $\psi(y)$ of the function $\varphi(y)$} %% подпись к рисунку
			\label{fig:fig2_2}
		\end{minipage}
	\end{center}
\end{figure}	


%При реализации алгоритма поиска областей притяжения локальных минимумов для построения дерева решений использовались алгоритмы из библиотеки OpenCV. OpenCV — это библиотека с открытым исходным кодом алгоритмов компьютерного зрения, обработки изображений и численных алгоритмов общего назначения. Более подробную информацию о дереве решений можно найти в \cite{Brahmbhatt2013}.
To implement the algorithm for identifying the regions of attraction for local minima to construct the decision tree algorithms from the OpenCV library were employed. OpenCV is an open-source library encompassing computer vision algorithms, image processing methods, and general-purpose numerical routines. Detailed information on decision trees may be found in \cite{Brahmbhatt2013}.


%После определения с помощью дерева решений области притяжения локального минимума, из точки ближайшего испытания, проведенного в этой области, запускается локальный метод. Мы использовали метод Хука--Дживса, который относится к классу методов нулевого порядка. Его правила вычисления представляют собой комбинацию исследовательского поиска (для выбора направления) и поиска в выбранном направлении \cite{Himmelblau72}.
Having identified a basin of attraction for a local minimum using the decision tree, a local method is launched from the nearest trial point conducted within that basin. We employed the Hooke-Jeeves method, classified as a zero-order technique. Its computational rules combine an exploratory search (for direction selection) with a search along the selected direction \cite{Himmelblau72}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Исследующий поиск определяется следующим образом: 
%\begin{itemize}[label=$\bullet$] 
%  \item Вначале определяется величина шага: она является различной для каждого координатного направления и может изменяться во время поиска. 
%  \item Шаг поиска считается успешным, если значение целевой функции в новой точке не превышает значение целевой функции в исходной точке. 
%  \item Если шаг является неуспешным, то нужно вернуться к исходной точке и сделать шаг в обратном направлении. 
%  \item После перебора всех N координатных направлений исследовательский поиск завершается; полученная точка является базовой точкой для поиска по образцу.
%\end{itemize}
%Поиск по образцу состоит из выполнения одного шага от полученной базовой точки вдоль линии, соединяющей ее с предыдущей базовой точкой.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The exploratory search is conducted as follows:
\begin{itemize}[label=$\bullet$] 
	\item Initially, a step size is determined; it can be different for each coordinate direction and can change during the search;
	\item A search step is considered successful if the value of the objective function at the new point does not exceed the value of the objective function at the starting point; 
	\item  If a step is unsuccessful, it is necessary to return to the starting point and take a step in the opposite direction; 
	\item After iterating through all $N$ coordinate directions, the exploratory search is completed; the resulting point is the base point for the pattern search.
\end{itemize}

Pattern search involves moving from the current base point along the line connecting it to the previous base point.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure} 
	\begin{center} 
		\begin{minipage}[!h]{1.0\linewidth} 
			\includegraphics[width=1\linewidth]{figure/fig1.pdf} 
			\caption{An example showing the iterations of the Hooke-Jeeves method} %%  подпись к рисунку 
			\label{fig:fig1} 
		\end{minipage} 
	\end{center} 
\end{figure}	


Fig. \ref{fig:fig1} visually demonstrates the local algorithm in action. Level lines are shown with filled circles representing successful steps and open circles indicating unsuccessful steps from the exploratory search. 


\section{Global search algorithm with local methods starting in parallel}\label{SecGSA}

%TODO
%Здесь нужны общие слова про параллельную программу с несколькими процессами, мастером и рабочими

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

One way to organize parallel computations on a supercomputer, which has a large number of computational cores and a high thread count per core, is to use synchronous parallel function evaluation.

%In the proposed approach, the parallelization scheme follows a ``master-worker'' principle. The master process executes the global search algorithm, accumulates search information, calculates the Lipschitz constant for the objective function based on this information, determines new trial points, and distributes them among the worker processes. The worker processes receive the trial points from the master process, evaluate the function at those points, and send the results back to the master process.

Our approach employs a ``master-worker'' parallelization strategy. The master process manages the global search algorithm, collects data to estimate the Lipschitz constant, computes new trial points, and assigning these points to the worker processes. The workers then evaluate the objective function at their assigned trial points and send the function values back to the master.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For the convenience of the subsequent algorithm description, we introduce the following notation.
\begin{itemize}
    \item By a \textit{trial} (or conducting a trial) we mean the calculation of the objective function value $z = f(x) = \varphi(y(x))$;
    \item By a \textit{trial point} we mean a point $y \in D$ and its preimage $x\in [0,1]$, where $y=y(x)$;
    \item We assume that the computational system used has $p + 1$ computing devices; trials will be conducted or a local method will be launched on $p$ of these devices (``workers''), while the main steps of the algorithm will be executed on one device (``master'');
    \item Let $U$ denote the set of trial points located in the basin of attraction of a minimum (local or global), from which a local method will be launched;
    \item Let $V$ denote the set of trial points which will be checked for the practicability of launching a local method from them; points that pass this check are placed into set $U$;
    \item Let $c$ is the number of points from which a local method needs to be launched at the current iteration.
    \item Let $n$ is the number of points at which trials will be conducted at the current iteration.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The master process (let it have the identifier 0) initiates $p$ parallel trials, each starting at a uniquely defined point $\{y\left(x^1\right),y\left(x^2\right),\ldots,y\left(x^p\right)\}$, where preimages $\{x^1,x^2,\ldots,x^p\}$ are from the interval $[0,1]$.
Two preimages are the boundary points $x^1=0, x^p=1$; the remaining preimages are interior points $x^i\in\left(0,1\right),i=2,\ldots,p-1$.
The sets $U$ and $V$ are empty.

Let us now assume that the algorithm has completed $K \geq 0$ iterations and $k \geq K$ trials have been performed.

%TODO
%Здесь нужно связующее предложение


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In each iteration, the algorithm selects several computation points and distributes them to processes in groups. MPI manages this, with process zero handling point distribution and result collection, but remaining computationally idle. Each remaining process receives a group of points, which are computed in parallel using the CPU.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{enumerate}
	
	\item Extract $c = \max(p, |U|)$ trial point coordinates $y^{k+1}=y\left(x^{k+1}\right),...,y^{k+c}=y\left(x^{k+c}\right)$ from the set $U$ (which contains trial point coordinates). If $c < p$, then first perform trials at $n = p - c$ points according to Steps~2--7; otherwise proceed directly to Step~8.
	
	\item Renumber (using subscripts) the points $x^i, 1\leq i\leq k$,  along with the boundary points of the interval $[0, 1]$ in increasing order of their coordinate values
	\begin{equation} 
		\label{agp1_sort} 	0=x_0<\ x_1<\ ...\ <x_{k+1}=1 	
	\end{equation} 
	and associate with them the values $z_i=f(x_i)$. 
	
	\item  Compute the current lower bound $M$ of the unknown Hölder constant $H$: 
	\begin{equation} 
		\label{agp2_mu} 	\mu=max\left\{\frac{|z_i-z_{i-1}|}{{{(x}_i-x_{i-1})}^{1/N}},\ i=1,\ldots,k\right\},\ M=\  \left\{\begin{matrix}r\mu,\ \mu>0,\\1,\ \mu=0,\\\end{matrix}\right.\ 	
	\end{equation} 
	where $r>1$ is the method parameter. Reliability of the algorithm is controlled by this parameter: higher values of $r$ guarantee finding the global minimum, while choosing a smaller value accelerates the convergence of the algorithm.
	
	\item  
	%For each interval $(x_{i-1},x_i), 1\leq i\leq k+1,$ compute a value $R(i)$, called \textit{the characteristic} of the interval, according to the following formulas:
	For every interval $(x_{i-1},x_i), 1\leq i\leq k+1,$ compute its \textit{characteristic} $R(i)$, as defined by the following formulas:
	\begin{equation} 
		\label{agp3_R1} R(1)=2\Delta_1-4\dfrac{z_1}{M}, \; R(k+1)=2\Delta_{k+1}-4\dfrac{z_k}{M}, 
	\end{equation} 
	\begin{equation} 
		\label{agp3_Ri} R(i)=\Delta_i+\dfrac{(z_i-z_{i-1})^2}{M^2\Delta_i}-2\dfrac{z_i+z_{i-1}}{M},1<i<k+1, 
	\end{equation} 
	where \(\Delta_i=(x_i-x_{i-1})^\frac{1}{N}\).
	
	\item   Sort the characteristics  $R\left(i\right),\ 1\leq i \leq k+1,$  in non-increasing order
	\begin{equation} 
		\label{agp4_R_sort} 	R\left(t_1\right)\geq\ R\left(t_2\right)\geq...\geq\ R\left(t_k\right)\geq\ R(t_{k+1}),\  
	\end{equation} 
and select $n$ intervals with indices $t_j, 1 \le j \le n$, that have the largest characteristic values.	
	
	\item Compute the coordinates of the new trial points $y'^{k+j}=y\left(x^{k+j}\right), \ 1\leq j\leq n$, whose preimages are  $x^{k+j}\in\left(x_{t_j-1},x_{t_j}\right)$, according to the following formula:
	\begin{equation}
		\label{agp5_x1}
		x^{k+j}=\frac{x_{t_j}+x_{t_j-1}}{2}-\mathrm{sign}\left(z_{t_j}-z_{t_j-1}\right)\frac{1}{2r}\left[\frac{\left|z_{t_j}-z_{t_j-1}\right|}{M}\right]^N.
	\end{equation}	
	

	
	\item Send to process $j$, $1 \leq j \leq n$, the coordinates of points $y'^{k+1}, \dots, y'^{k+n}$ and notify the process that it needs to perform a single trial.
	
	\item Send to processes $h$, $1 \leq h \leq c$, the coordinates of points $y^{k+1}, \dots, y^{k+c}$ and notify the process that it needs to run a local method.
	
	\item Receive trial results from all $p$ processes and add them to the search information.
	
	\item Add the trial points $y'^{k+1}=y\left(x^{k+1}\right) , \dots, y'^{k+n}=y\left(x^{k+n}\right)$  to the set $V$.
	
	\item If the number of trials is not large enough, namely, if $k < 100 N$, go to Step~1.
	
	
	\item Build a decision tree using all the accumulated search information and obtain the approximation $\psi(y)$.
	
	\item 
	%If the decision tree is being used for the first time, construct a uniform grid
	Upon the first application of the decision tree, generate a uniform grid
	\begin{displaymath} 
		Y'=\{ y'\in R^N:\ a_i\le y'_i \le b_i,\ 1\le i\le N \},
	\end{displaymath} 
	%where the number of grid nodes is a parameter of the method.
	where the density of grid points is governed by a user-defined parameter.
	
	
	\item Compute the piecewise-constant approximation $\psi(y)$ of the function $\varphi(y)$ using the decision tree.% $Z' = \{ z' = \psi(y'), y' \in Y' \}$.
	
	\item For all points $y' \in V$, find the points $y'_q$ closest to $y'$.
		If no point $y'_q$ has a value smaller than $y'$, add $y'$ to the set $U$.
		Clear the set $V$.
	
	\item Check the stopping criteria.
	
\end{enumerate}

The next $p$ trials are carried out in parallel at the points $x^{k+j},\ 1\leq j\leq p$,  computed using formula  (\ref{agp5_x1}). Upon completion of these trials, their results are stored in the search information database, and the algorithm proceeds to compute new trial points. Note that, generally, the trial process in applied optimization problems is far more computationally expensive than computing the coordinates of a point.

The algorithm terminates if the inequality \(\Delta_{t_j} < \delta \) is true for at least one value of $t_j,\ 1\le\ j\le\ p$, from (\ref{agp4_R_sort}).
%This stopping condition (along with the usual criterion of limiting the number of iterations, common for iterative methods) is used in applied optimization problems where the global minimizer $y^*$ is not known a priori.
In applied optimization problems, where the global minimizer $y^*$ is typically not known in advance, this stopping condition complements the standard practice of limiting iterations.


When solving test problems where the global minimizer $y^*$ is known, a stopping criterion based on reaching a neighborhood of the global minimum point can be used. In this case, the algorithm terminates if the inequality $\left\|y(x_{t_j})-\ y^\ast\right\| < \delta$ is true for one value of $t_j,\ 1\le\ j\le\ p$ from (\ref{agp4_R_sort}).

%The values  
%\begin{equation} 
%	f_k^*=\min_{1\leq i \leq k}f(x_i), \; x_k^*=arg \min_{1\leq i \leq k}f(x_i),
%\end{equation} 
%are taken as the final estimate of the global minimum for the problem under consideration.

See \cite{Strongin2000,Barkalov2016} for the justification of this computational approach. Techniques for handling inequality constraints and using objective function derivative are discussed in \cite{Barkalov2002, Gergel1997, Barkalov2023, Gegrel2021}.





% переводить до этого раздела
\section{Computational experiments}\label{SecR}

%Численные эксперименты проводились на суперкомпьютере Лобачевский. Каждый узел имел по два процессора Intel Sandy Bridge E5-2660 2,2 ГГц, 64 Гб оперативной памяти.
The Lobachevsky supercomputer was used for the numerical experiments. Each supercomputer node featured two Intel Sandy Bridge E5-2660 2.2 GHz processors and 64 GB of RAM.
%В экспериментах использовался генератор тестовых задач GKLS, который может генерировать задачи многоэкстремальной оптимизации с известными свойствами: точка глобального минимума, количество локальных минимумов и др.
The GKLS test problem generator \cite{Sergeyev2006} was used in the experiments.
%This generator can create multi-extremal optimization problems with known properties, such as the location of the global minimum, the number of local minima, etc.
The generator produces complex optimization problems with multiple local minima and predictable characteristics, like the global minimum's location and the number of local minima.

%Ниже представлены результаты работы синхронного параллельного алгоритма глобального поиска  с использованием дерева решений для нахождения областей притяжения локальных минимумов. Численное сравнение проводилось на двух классах функций GKLS (Simple и Hard из \cite{Sergeyev2006}) размерностей 2, 3, 4 и 5. Эти два класса отличаются размером области притяжения точки глобального минимума. Для простого класса задач радиус области притяжения в три раза больше, чем для сложного.

The following results demonstrate the performance of a synchronous parallel global search algorithm that uses decision trees to identify basins of attraction for local minima. The algorithm was tested on Simple and Hard classes of GKLS functions from \cite{Sergeyev2006} with dimensions $N=2,...,5$, which differ in the size of their global minimum's basin of attraction. The Simple class has a basin three times larger than the Hard class.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Algorithms terminated when current trial point was within the $\varepsilon$-vicinity of the true global minimizer. Tables \ref{tab:1} and \ref{tab:2} show the average time and speedup relative to a sequential run, achieved by the algorithms when solving optimization problems. Figure \ref{fig:fig4} shows the speedup in terms of the number of iterations. Parallelization was implemented using MPI, and experiments were conducted on $P = 1, 8, 16, 32, 64$ processes.


\begin{figure} 
	\begin{center} 
		\begin{minipage}[h]{0.9\linewidth} 
			\includegraphics[width=1\linewidth]{figure/fig6.pdf} 
			\caption{Iteration speedup} %%  подпись к рисунку 
			\label{fig:fig4} 
		\end{minipage} 
	\end{center} 
\end{figure}	

\begin{table}
	\caption{Average time to solve the problem}
	\label{tab:1}
	\center
	\begin{tabular}{cccccccccccc}
		\hline\noalign{\smallskip}
		$P$ & \multicolumn{2}{c}{ $N=2$ } & & \multicolumn{2}{c}{$N=3$} & & \multicolumn{2}{c}{$N=4$} & & \multicolumn{2}{c}{$N=5$}  \\
		\noalign{\smallskip} \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12} \noalign{\smallskip}
		& \textit{Simple} & \textit{Hard} & & \textit{Simple} & \textit{Hard} & & \textit{Simple} & \textit{Hard} & & \textit{Simple} & \textit{Hard}  \\
		\noalign{\smallskip} \hline \noalign{\smallskip}
		1   & 6.35 & 12.71 & & 5.92 & 14.19 & & 31.88 & 66.29  & & 53.77 & 324.04 \\
		8   & 0.57 & 1.98  & & 1.63 & 5.59  & & 9.44  & 34.93  & & 6.62 & 52.42 \\
		16  & 1.27 & 1.71  & & 1.17 & 2.10  & & 4.71  & 8.53   & & 3.38 & 17.55 \\
		32  & 0.95 & 0.87  & & 0.38 & 0.75  & & 4.67  & 4.95   & & 1.99 & 11.48 \\
		64  & 0.21 & 1.65  & & 0.41 & 0.60  & & 5.51  & 2.93   & & 1.70 & 10.60 \\
		\noalign{\smallskip}\hline
	\end{tabular}
\end{table}

\begin{table}
	\caption{Time speedup}
	\label{tab:2}
	\center
	\begin{tabular}{cccccccccccc}
		\hline\noalign{\smallskip}
		$P$ & \multicolumn{2}{c}{ $N=2$ } & & \multicolumn{2}{c}{$N=3$} & & \multicolumn{2}{c}{$N=4$} & & \multicolumn{2}{c}{$N=5$}  \\
		\noalign{\smallskip} \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12} \noalign{\smallskip}
		& \textit{Simple} & \textit{Hard} & & \textit{Simple} & \textit{Hard} & & \textit{Simple} & \textit{Hard} & & \textit{Simple} & \textit{Hard}  \\
		\noalign{\smallskip} \hline \noalign{\smallskip}
		8  & 11.19 & 6.42  & & 3.98  & 4.01  & & 4.20 & 3.52  & & 10.80 & 10.28 \\
		16 & 5.02  & 7.43  & & 5.07  & 6.76  & & 6.77 & 7.77  & & 15.93 & 18.46 \\
		32 & 13.59 & 16.45 & & 14.97 & 22.41 & & 7.27 & 15.64 & & 31.96 & 38.76 \\
		64 & 30.42 & 7.70  & & 14.49 & 23.48 & & 5.79 & 22.61 & & 31.65 & 30.57 \\
		\noalign{\smallskip}\hline
	\end{tabular}
\end{table}


%\begin{table}[!ht]
%	\caption{Ускорениепо итерациям}
%	\label{tab:3}
%	\centering	
%	\begin{tabular}{|l|l|l|l|l|}
	%		\hline
	%		2 & 8 mpi & 16 mpi & 32 mpi & 64 mpi  \\ \hline
	%		hard & 37,61 & 49,11 & 50,04 & 34,58  \\ \hline
	%		simple & 52,06 & 19,72 & 63,02 & 307,59  \\ \hline
	%		3 & ~ & ~ & ~ &   \\ \hline
	%		hard & 15,03 & 92,04 & 47,28 & 89,74  \\ \hline
	%		simple & 10,25 & 44,70 & 44,29 & 76,87  \\ \hline
	%		4 & ~ & ~ & ~ &   \\ \hline
	%		hard & 38,14 & 106,64 & 58,40 & 117,56  \\ \hline
	%		simple & 30,75 & 79,84 & 29,41 & 40,43  \\ \hline
	%		5 & ~ & ~ & ~ &   \\ \hline
	%		hard & 162,66 & 269,09 & 201,95 & 206,94  \\ \hline
	%		simple & 46,13 & 176,70 & 98,42 & 191,00  \\ \hline
	%	\end{tabular}
%\end{table}



%Как видно из приведенных данных, присутствует ускорение по времени для задач любой размерности, при этом в целом значение ускорения не линейное, но встречается и сверхлинейное ускорение. В данных задачах большое значение имеет что, как быстро алгоритм глобального поиска поставит достаточное для дерева решения количество точек в окрестности глобального минимума, после чего локальный метод быстро находит оптимум, из за этого возникает сверхлинейное ускорение. Низкое ускорение в остальных случаях, в большей степени обусловлено тем, что алгоритм оптимизации является синхронным и на некоторых итерациях происходит ожидание завершения локального поиска, также значительным оказалось влияние накладных расходов на передачу точек между процессами. 

As can be seen from the presented data, there is a time speedup for problems of all dimensions. While the overall speedup is not linear, superlinear speedup does occur. In these problems, it is crucial how quickly the global search algorithm places enough points in the neighborhood of the global minimum for the decision tree; after this, the local method quickly finds the optimum, resulting in superlinear speedup. Lower speedup in other cases is primarily due to the synchronous nature of the optimization algorithm, leading to waiting for the completion of local searches in some iterations. The overhead of transferring points between processes also had a significant impact.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\section{Conclusion} 

%Первая версия заключения

%На данный момент в результате проделанной работы удалось совместить алгоритм глобального поиска с эффективным локальным уточнением. В отличие от известных мультистартовых схем решение о запуске локального метода принимается с помощью дерева решений. Использование такой комбинации методов позволяет значительно ускорить работу алгоритма.
%Параллельная версия алгоритма сохраняет свойства своего последовательного прототипа, что было подтверждено численными экспериментами по решению серии из нескольких сотен задач различной размерности. Предлагаемая схема позволяет использовать преимущества как распараллеливания, так и быстрого поиска локальных экстремумов.
%Помимо использования деревьев решений для выявления областей притяжения локальных экстремумов многоэкстремальных функций, мы также планируем использовать методы машинного обучения для разделения переменных решаемой задачи. Во многих прикладных задачах оптимизации зависимость целевой функции от некоторых параметров бывает либо линейной, либо одномодальной. Выделение таких переменных в специальную группу и решение задачи с помощью параллельной схемы рекурсивной оптимизации \cite{Barkalov2020_1} позволяет сократить время решения задачи на порядки по сравнению с использованием глобального поиска сразу по всем переменным.

%Currently, the work done has made it possible to combine a global search algorithm with effective local refinement. Unlike known multi-start schemes, the decision to launch the local method is made using a decision tree. The use of such a combination of methods significantly speeds up the algorithm.
%The parallel version of the algorithm retains the properties of its sequential prototype, which was confirmed by numerical experiments on solving a series of several hundred problems of various dimensions. The proposed scheme allows us to exploit the advantages of both parallelization and rapid local extrema search.
%In addition to using decision trees to identify the basins of attraction of local extrema of multimodal functions, we also plan to use machine learning methods to separate the variables of the problem being solved. In many applied optimization problems, the dependence of the objective function on some parameters is either linear or unimodal. Identifying such variables into a special group and solving the problem using a parallel recursive optimization scheme \cite{Barkalov2020_1} can reduce the solution time by orders of magnitude compared to using global search across all variables at once.


%Вторая версия заключения


%В данной работе рассматриваются задачи липшицевой глобальной оптимизации и параллельные алгоритмы для их решения. Целевая функция f(x) в таких задачах предполагается многоэкстремальной и заданной в виде черного ящика. Основную сложность при решении задач глобальной оптмизации представляет определение свойств целевой функции в различных подобластях области поиска и их использование в правилах алгоритма. Так, важной задачей является выявление областей притяжения локальных экстремумов целевой функции. Ведь даже если метод глобальной оптмизации провел испытание в области притяжения локального экстремума, который одновременно является глобальным, то для нахождения решения задачи с высокой точностью методу может потребоваться значительное количество дополнительных испытаний.

This work considers Lipschitz global optimization problems and parallel algorithms for solving them. The objective function $f(x)$ in such problems is assumed to be multimodal and given as a ``black box''. The main difficulty in solving global optimization problems lies in determining the properties of the objective function in different subregions of the search domain and using them in the algorithm's rules. Thus, an important task is identifying the basins of attraction of local minimizers of the objective function. Indeed, even if a global optimization method performs a trial within the basin of attraction of a local minimizer that is also the global minimizer, the method may still require a significant number of additional trials to find the solution with high accuracy.

%Традиционным приемом преодоления этой трудности является прекрашение (в некоторый момент) глобального поиска с последующим стартом локального алгоритма, который улучшает полученное решение на заключительной фазе поиска. При этом подходе возникают сложности, связанные с сопряжением глобальной и локальной процедур. Одной из них является вопрос об определении момента остановки глобального алгоритма: преждевременная остановка может привести к потере глобального решения, поздняя - замедляет поиск. Другая проблема этого подхода заключается в том, что локальная информация о поведении целевой функции используется только в окрестности глобального минимума.

A traditional approach to overcoming this difficulty is to terminate the global search at some iteration and subsequently start a local algorithm, which refines the obtained solution in the final phase of the search. This approach raises challenges related to the coupling of global and local procedures. One of these is the question of determining when to stop the global algorithm: premature termination can lead to the loss of the global solution, while late termination slows down the search. Another problem with this approach is that local information about the behavior of the objective function is used only in the vicinity of the global minimizer.

%В результате проделанной работы у нас получилось совместить алгоритм глобального поиска с эффективным локальным уточнением. В отличие от известных мультистартовых схем, в которых запуск локального метода происходит из случайных точек области поиска, решение о старте процедуры локального уточненя принимается с помощью дерева решений. Использование такой комбинации методов позволяет значительно ускорить работу алгоритма за счет быстрого нахождения локально-оптимальных решений. 

As a result of the work done, we were able to combine a global search algorithm with efficient local refinement. Unlike known multistart schemes, where the local method is launched from random points in the search domain, the decision to start the local refinement procedure is made using a decision tree. This combination of methods significantly speeds up the algorithm by enabling the rapid identification of locally optimal solutions.

%На основе исходного последовательного алгоритма был разработан его параллельный вариант, ориентированный на использование в вычислительных системах с распределенной памятью. Алгоритм использует концепцию мастер-рабочие и реализован с использованием технологии MPI на языке С++. С использованием параллельного алгоритма были проведены вычислительные эксперименты, в ходе которых были решены несколько сотен многомерных многоэкстрмальных задач. Результаты проведенных экспериментов продемонститровали как сокращение времени работы (за счет распралаллеливания), так и уменьшение числа необходимых поисковых испытаний (за счет быстрого локального уточнения).

A parallel version of the original sequential algorithm was developed, designed for use in distributed memory computing systems. The algorithm employs a master-worker paradigm and is implemented using MPI technology in C++. Computational experiments were conducted using the parallel algorithm, solving several hundred multidimensional multimodal problems. The results demonstrated both a speedup in runtime (due to parallelization) and a decrease in the number of required search trials (due to the fast local refinement).

%Конечно, использование методов машинного обучения в задачах глобальной оптмизации не ограничивается лишь выявлением областей притяжения локальных экстремумов. Одним из интересных свойст прикладных оптимизационных задачах большой размерности, возникающих при идентификации параметров сложных математических моделей, является локальное влияние части переменных на целевую функцию. Зависимость лишь от небольшого числа переменных носит многоэкстремальный характер. Однако какие именно переменные влияют глобально, а какие - локально, заранее не известно. С помощью методов машинного обучения можно выделить переменные, влияющие локально, в отдельную группу, и организовать решение задачи с помощью схемы вложенной (рекурсивной) оптимизации \cite{Barkalov2020_1}. Использование локальных методов на внутреннем уровне рекурсии и глобальных - на внешнем позволит значительно сократить общее время решения задачи по сравнению с использованием только глобального поиска.

Naturally, the application of machine learning techniques to global optimization problems extends beyond locating basins of attraction for local optima. One notable characteristic of multidimensional applied optimization problems, particularly those arising from the identification of parameters in complex mathematical models, is the local influence of some variables on the objective function. Dependence on only a small number of variables is multimodal. However, which variables have a global influence versus a local one is not known a priori. Machine learning methods can be used to isolate variables that have a local impact into a separate group, and then solve the problem using a nested (recursive) optimization scheme \cite{Barkalov2020_1}. Utilizing local methods at the inner level of recursion and global methods at the outer level can significantly reduce the overall solution time compared to using global search alone.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{credits}
	\subsubsection{\ackname} This study was supported by the Ministry of Science and Higher Education of the Russian Federation, project no. FSWR-2023-0034.
	
	\subsubsection{\discintname}
	The authors have no competing interests to declare that are	relevant to the content of this article.
\end{credits}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%
% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\bibliography{bibliography}{}











\end{document}
