% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%

\usepackage[utf8]{inputenc} % кодировка
\usepackage[english, russian]{babel} % Русские и английские переносы
\usepackage{cite}              % для корректного оформления литературы
\usepackage{enumitem}
\usepackage{amsmath} 


\begin{document}
%
\title{Parallel global optimization algorithm employing decision trees for launching local methods}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Konstantin Barkalov\orcidID{0000-0001-5273-2471} \and
Ilya Lebedev\orcidID{0000-0002-8736-0652} \and
Dmitry Silenko}
%
\authorrunning{K. Barkalov et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Lobachevsky State University of Nizhni Novgorod, Nizhni Novgorod, Russia}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
%В данной работе разработан подход к параллельному запуску локальных методов при решении задачи глобальной оптимизации с помощью алгоритма глобального поиска. Целевая функция задачи задана как черный ящик; предполагается, что она удовлетворяет условию Липшица с априори неизвестной константой. В работе рассматривается метод выбора окрестности локальных экстремумов целевой функции на основе анализа накопленной поисковой информации. Проведение такого анализа с использованием методов машинного обучения позволяет принять решение о параллельном запуске локальных методов, что позволяет ускорить сходимость алгоритма. Это предположение было подтверждено результатами численных экспериментов, демонстрирующих ускорение при решении серии тестовых задач.

This paper presents an approach for the parallel launch of local optimization methods within a global search algorithm framework, aimed at solving global optimization problems. The objective function is treated as a black-box and is assumed that it satisfies the Lipschitz condition with an unknown constant. The proposed method focuses on selecting the region of attraction of local extrema of the objective function based on the analysis of accumulated search data. Employing machine learning techniques for this analysis enables informed decisions regarding the parallel execution of local methods, leading to improved algorithm convergence. The effectiveness of this approach is validated through numerical experiments, which demonstrate accelerated performance on a set of benchmark problems.


\keywords{
Global optimization $\cdot$
Multi-extremal functions $\cdot$
Parallel computing $\cdot$
Machine learning $\cdot$
Decision tree $\cdot$}
\end{abstract}
%
%
%



\section{Introduction}

%В настоящей работе рассматриваются параллельные алгоритмы решения многомерных задач глобальной оптимизации. Такие задачи часто возникают в тех случаях, когда необходимо подобрать значения параметров исследуемой математической модели, при которых результаты моделирования лучше всего соответствуют экспериментальным данным. Для решения задач этого класса известно множество алгоритмов: от метаэвристических, основанных на идее случайного поиска \cite{Ferreiro2013,Garcia2014,Langdon2011}, до детерминированных алгоритмов, гарантирующих сходимость к глобальному минимуму \cite{Evtushenko2009,He2008,Paulavicius2011}.
This paper investigates parallel algorithms for solving multidimensional global optimization problems These problems commonly occur when optimizing the parameters of a mathematical model to achieve the best possible agreement between simulation results and experimental observations. Numerous algorithms have been developed to address this class of problems, varying from metaheuristic techniques based on the idea of random search \cite{Ferreiro2013,Garcia2014,Langdon2011} to deterministic algorithms designed to guarantee convergence to a global minimum \cite{Evtushenko2009,He2008,Paulavicius2011}.


%Поскольку в реальных задачах глобальной оптимизации каждое вычисление значения функции (далее \textit{trial}) является операцией, требующей больших вычислительных затрат, количество таких попыток приходится сокращать. Это может быть достигнуто за счет преднамеренного выбора вариантов в ходе поиска оптимального решения, отсекая бесперспективные поисковые под области и исследуя только те, в которых можно найти решение задачи. Алгоритм глобального поиска (АГП) основан на этой идее \cite{Strongin2000}. В настоящей работе мы попытались объединить АГП и метод локальной оптимизации \cite{Nocedal, Kelley}  (метод поиска по образцу Хука-Дживса \cite{HookJeeves}), чтобы уменьшить количество выполняемых испытаний. Решение о запуске локального метода будет приниматься с использованием дерева решений.
In real-world global optimization problems, each function evaluation (hereafter referred to as a \textit{trial}) is a computationally expensive operation, making it necessary to reduce the number of such trials. This may be achieved by the purposeful selection of alternatives during the course of optimization, rejecting unpromising search sub-regions and exploring only those where a solution can be found. The Global Search Algorithm (GSA) is based on this idea \cite{Strongin2000}. In this work, we attempted to combine the GSA and a local optimization method \cite{Nocedal, Kelley} (the Hooke-Jeeves pattern search method \cite{HookJeeves}) to reduce the number of trials performed. The decision to initiate the local method will be made using a decision tree.

\section{Problem statement and solution strategies}\label{SecA}

We consider the problem of finding the global minimum of the function $\varphi(y)$ over a hyperinterval $D$, 
\begin{equation} \label{sec:problem}   
	\varphi(y^*) = min\{\varphi(y):y\in D\}, D = \{y \in R^N : a_i \leq y_i \leq b_i, 1 \leq i \leq N \},
\end{equation}
where $a,b \in R$ are given vectors.

We assume that the function satisfies the Lipschitz condition 
\begin{displaymath} 
	|\varphi(y_1)-\varphi(y_2)|\leq L\parallel y_1-y_2 \parallel ,y_1,y_2 \in D, 0<L< \infty, 
\end{displaymath}
with an a priori unknown constant $L$, which corresponds to a bounded change in the function's values for a bounded change in the argument.
This assumption can be viewed (in the context of real-world problems) as reflecting the limited power generating changes in the simulated system.

The numerical solution of the problem (\ref{sec:problem}) reduces to constructing an estimate $y_k^\ast\in D$ that corresponds to some notion of proximity to a point $y^\ast$ (e.g., ${ ||y^\ast-y}_k^\ast||\le\ \varepsilon$, where $\varepsilon\geq0$ is a given accuracy) based on a finite number $k$ of computed values of the objective function. With respect to the class of problems considered, we assume that the objective function $\varphi(y)$ can be algorithmically defined through the execution of a subroutine.

When solving multidimensional problems, dimensionality reduction is employed (i.e., reducing the multidimensional problem to an equivalent one-dimensional one) using Peano curves. These curves allow transforming the multidimensional optimization problem in the region $D$ into a one-dimensional minimization problem on the interval [0, 1].
\begin{displaymath}
	\varphi(y(x^\ast))\ =\min\{\varphi(y(x)): x\in [0,1]\},
\end{displaymath}
where function $\varphi(y(x^\ast))$ satisfies the more general Hölder condition
\begin{displaymath}
	\left|\varphi (y \left(x_1\right))- \varphi (y \left(x_2\right)\right )|\le\ H\left|x_1-x_2\right|^\frac{1}{N},\ x_1,\ x_2\epsilon[0,1].
\end{displaymath} 

Consequently, rather than addressing the original problem of minimizing the function $\varphi(y)$ within the domain $D$, we can consider minimizing the one-dimensional function $f(x)=\varphi(y(x))$, subject to the Hölder condition for $x \in [0, 1]$.


%Чтобы определить, в какой момент лучше запускать локальный метод в рамках глобального поиска, воспользуемся алгоритмом, основанным на дереве решений. На первом этапе поиска происходит накопление поисковой информации в виде результатов выполненных испытаний. На основе результатов испытаний проводилось обучение дерева решений, позволяющего получать кусочно-постоянную аппроксимацию целевой функции и на ее основе прогнозировать поведение целевой функции. 
To identify when to best launch a local method  within the framework of a global search, we employ an algorithm based on a decision tree. The first stage of the search involves accumulating search information in the form of results from performed trials. Based on these trial results, a decision tree is trained, allowing us to obtain a piecewise-constant approximation of the objective function and, consequently, to predict the behavior of the objective function.

%Дерево решений — это инструмент для автоматизированного анализа больших массивов данных, применяемый в машинном обучении. Его можно использовать как для решения задач классификации, так и для регрессии. В случае построения регрессии каждому листу дерева присваивается константа. Поэтому полученная аппроксимирующая функция является кусочно-постоянной. В нашем случае дерево решений строит функцию $\psi(y)$ как кусочно-постоянную аппроксимацию $\varphi(y)$ в области поиска. Обозначим значение, вычисленное деревом решений в точке $y$, как $z' = \psi(y)$.
A decision tree is a tool for automated analysis of large datasets used in machine learning. It can be used for both classification and regression problems. In the case of regression, each leaf of the tree is assigned a constant. Therefore, the resulting approximating function is piecewise-constant. In our case, the decision tree constructs a function $\psi(y)$ as a piecewise-constant approximation of $\varphi(y)$ in the search space. Let $z' = \psi(y)$ denote the value computed by the decision tree at point $y$.

Fig. \ref{fig:fig2} shows the graph of the objective function $\varphi(y)$; Fig. \ref{fig:fig2_2} presents the corresponding approximation $\psi(y)$ constructed using a decision tree.
\begin{figure}
	\begin{center}
		\begin{minipage}[h]{0.7\linewidth}
			\includegraphics[width=1\linewidth]{figure/fig5.png}
			\caption{Graph of the benchmark objective function $\varphi(y)$} %% подпись к рисунку
			\label{fig:fig2}
		\end{minipage}
	\end{center}
\end{figure}	

\begin{figure}
	\begin{center}
		\begin{minipage}[h]{0.7\linewidth}
			\includegraphics[width=1\linewidth]{figure/fig4.png}
			\caption{Graph of the piecewise-constant approximation $\psi(y)$ of the function $\varphi(y)$} %% подпись к рисунку
			\label{fig:fig2_2}
		\end{minipage}
	\end{center}
\end{figure}	


%При реализации алгоритма поиска областей притяжения локальных минимумов для построения дерева решений использовались алгоритмы из библиотеки OpenCV. OpenCV — это библиотека с открытым исходным кодом алгоритмов компьютерного зрения, обработки изображений и численных алгоритмов общего назначения. Более подробную информацию о дереве решений можно найти в \cite{Brahmbhatt2013}.
To implement the algorithm for identifying the regions of attraction for local minima to construct the decision tree algorithms from the OpenCV library were employed. OpenCV is an open-source library encompassing computer vision algorithms, image processing methods, and general-purpose numerical routines. Detailed information on decision trees may be found in \cite{Brahmbhatt2013}.


%После определения с помощью дерева решений области притяжения локального минимума, из точки ближайшего испытания, проведенного в этой области, запускается локальный метод. Мы использовали метод Хука--Дживса, который относится к классу методов нулевого порядка. Его правила вычисления представляют собой комбинацию исследовательского поиска (для выбора направления) и поиска в выбранном направлении \cite{Himmelblau72}.
Having identified a basin of attraction for a local minimum using the decision tree, a local method is launched from the nearest trial point conducted within that basin. We employed the Hooke-Jeeves method, classified as a zero-order technique. Its computational rules combine an exploratory search (for direction selection) with a search along the selected direction \cite{Himmelblau72}.

\begin{figure} 
	\begin{center} 
		\begin{minipage}[h]{0.8\linewidth} 
			\includegraphics[width=1\linewidth]{figure/fig1.pdf} 
			\caption{An example showing the iterations of the Hooke-Jeeves method} %%  подпись к рисунку 
			\label{fig:fig1} 
		\end{minipage} 
	\end{center} 
\end{figure}	


Fig. \ref{fig:fig1} visually demonstrates the local algorithm in action. Level lines are shown with filled circles representing successful steps and open circles indicating unsuccessful steps from the exploratory search. 


\section{Многомерный параллельный алгоритм глобального поиска}\label{SecGSA}

На начальном этапе мастер-процесс (пусть он имеет идентификатор $0$) параллельно запускает $p$ испытаний в $p$ разных точках $\{y\left(x^1\right),y\left(x^2\right),\ldots,y\left(x^p\right)\}$ отрезка $[0,1]$.
Прообразами двух точек являются граничные $x^1=0,x^p=1$; остальные — внутренние точки $x^i\in\left(0,1\right),i=2,\ldots,p-1$.

Теперь предположим, что алгоритм уже сделал $K\geq0$ итераций и проведено $k$ испытаний.


\begin{enumerate}
	
	\item Извлекаем из множество $U$ (координат точек испытания) $c = max(p, |U|)$ координат точек испытания $y^{k+1}=y\left(x^{k+1}\right) ... y^{k+c}=y\left(x^{k+c}\right)$. Если $c < p$ то сначала вычисляем $n = p - c$ точек согласно шагам 2 - 7, иначе сразу переходим к шагу 8.	
	
	\item Перенумеруем (по нижними индексами) точки $x^i, 1\leq i\leq k$, а также граничные точки отрезка $[0,1]$ в порядке возрастания координаты  
	\begin{equation} 
		\label{agp1_sort} 	0=x_0<\ x_1<\ ...\ <x_{k+1}=1. 	
	\end{equation} 
	и сопоставить их со значениями $z_i=f(x_i)$. 
	
	\item  Вычислить текущую нижнюю оценку $M$ неизвестной константы Гельдера $H$: 
	\begin{equation} 
		\label{agp2_mu} 	\mu=max\left\{\frac{|z_i-z_{i-1}|}{{{(x}_i-x_{i-1})}^{1/N}},\ i=1,\ldots,k\right\},\ M=\  \left\{\begin{matrix}r\mu,\ \mu>0,\\1,\ \mu=0,\\\end{matrix}\right.\ 	
	\end{equation} 
	где $r>1$ — параметр алгоритма. Этот параметр управляет надежностью алгоритма: более высокие значения $r$ гарантируют гарантированное нахождение глобального минимума, выбор меньшего значения ускоряет сходимость алгоритма.
	
	\item  Для каждого интервала $(x_{i-1},x_i), 1\leq i\leq k+1,$ вычислить значение $R(i)$, называемое \textit{характеристикой} интервала, по формулам
	
	\begin{equation} 
		\label{agp3_R1} R(1)=2\Delta_1-4\dfrac{z_1}{M}, \; R(k+1)=2\Delta_{k+1}-4\dfrac{z_k}{M}, 
	\end{equation} 
	
	\begin{equation} 
		\label{agp3_Ri} R(i)=\Delta_i+\dfrac{(z_i-z_{i-1})^2}{M^2\Delta_i}-2\dfrac{z_i+z_{i-1}}{M},1<i<k+1, 
	\end{equation} 
	
	где \(\Delta_i=(x_i-x_{i-1})^\frac{1}{N}\).
	
	\item   Упорядочить характеристики $R\left(i\right),\ 1\leq i \leq k+1,$ в порядке не возрастания
	\begin{equation} 
		\label{agp4_R_sort} 	R\left(t_1\right)\geq\ R\left(t_2\right)\geq...\geq\ R\left(t_k\right)\geq\ R(t_{k+1}),\  
	\end{equation} 
	и выделить $n$ интервалов с индексами $t_j,\ 1\le\ j\le\ n$  с наибольшими значениями характеристик.	
	
	\item Вычислить новые координаты точки испытания $y'^{k+j}=y\left(x^{k+j}\right), \ 1\leq j\leq n$ прообразом которого является $x^{k+j}\in\left(x_{t_j-1},x_{t_j}\right)$, в соответствии с формулой
	\begin{equation}
		\label{agp5_x1}
		x^{k+1}=\frac{x_t+x_{t-1}}{2}-\mathrm{sign}\left(z_t-z_{t-1}\right)\frac{1}{2r}\left[\frac{\left|z_t-z_{t-1}\right|}{M}\right]^N.
	\end{equation}	
	
	
	\item Отправить на процесс $j, \ 1\leq j\leq n$ координату $y'^{k+1} ... y'^{k+n}$  и сообщаем процессу, что нужно произвести одиночное испытание.
	
	\item Отправить на процессы $h, \ 1\leq h\leq c$ координаты точек $y^{k+1} ... y^{k+c}$ и сообщаем процессу, что нужно запустить локальный метод.
	
	\item Получить со всех $p$ процессов значения испытаний и добавить их в поисковую информацию.
	
	\item Добавить точки испытаний $y'^{k+1}=y\left(x^{k+1}\right) ... y'^{k+n}=y\left(x^{k+т}\right)$ к множеству $V$.
	
	\item Если $k\ <\ 100 N$, перейти к шагу 1.
	
	
	\item Строим дерево решений по всей накопленной поисковой информации, получаем аппроксимирующую функцию $\psi(y)$.
	
	\item Если дерево решений используется впервые, постройте равномерную сетку
	\begin{displaymath} 
		Y'=\{ y'\in R^N:\ a_i\le y'_i \le b_i,\ 1\le i\le N \},
	\end{displaymath} 
	где количество узлов сетки является параметром метода.
	
	\item Вычислить кусочно постоянную аппроксимацию функции $\psi(y)$, по дереву решений : $Z' = \{ z'=  \psi(y'), y' \in Y'\}$
	
	\item Для всех точек $y'\in V$ найдите точки $y'_q$, ближайшие к $y'$,
	
	Если ни одна точка $y'_q$ не имеет меньшего значения, чем $y'_q$, поместить $y'$ в множество $U$
	
	Очистить множество $V$.
	
	\item Проверка критериев остановки
	
	
\end{enumerate}

Следующие $p$ испытания выполняются параллельно в точках $x^{k+j},\ 1\leq j\leq p$, вычисляемых по формуле (\ref{agp5_x1}). По завершении испытаний результаты этих испытаний сохраняются в базе поисковой информации, а алгоритм переходит к вычислению новых точек испытания.
Отметим, что, как правило, процесс испытания в прикладных оптимизационных задачах гораздо более затратный по вычислительным ресурсам по сравнению с вычислением координаты точки.

Алгоритм останавливается в том случае, если выполняется условие \(\Delta_{t_j} < \varepsilon\) хотя бы для одного значения $t_j,\ 1\le\ j\le\ p$, из (\ref{agp4_R_sort} ). Этот критерий остановки (наряду с обычным для итерационных методов критерием ограничения числа выполняемых итераций) используется в прикладных задачах оптимизации, в которых точка глобального минимума $y^*$ априори неизвестна.

При решении тестовых задач, в которых известна точка глобального минимума $y^*$, можно использовать критерий остановки и по попаданию в окрестность глобального минимума. В этом случае метод останавливается, если выполняется условие $\left\|y(x_{t_j})-\ y^\ast\right\| < \varepsilon$ для одного $t_j,\ 1\le\ j\le\ p$ из (\ref{agp4_R_sort}).

В качестве окончательной оценки глобального минимума рассматриваемой задачи берется значение 
\begin{equation} 
	f_k^*=\min_{1\leq i \leq k}f(x_i), \; x_k^*=arg \min_{1\leq i \leq k}f(x_i). 
\end{equation} 


Обоснование такого способа организации вычислений см. в \cite{Strongin2000,Barkalov2016}. Модификации с учетом наличия ограничений-неравенств, а также информация о производной целевой функции представлены в \cite{Barkalov2002,Gergel1997}.





% переводить до этого раздела
\section{Численные эксперименты}\label{SecR}


Численные эксперименты проводились на суперкомпьютере Лобачевского. Каждый узел имел по два процессора Intel Sandy Bridge E5-2660 2,2 ГГц, 64 Гб оперативной памяти.

В экспериментах использовался генератор тестовых задач GKLS, который может генерировать задачи многоэкстремальной оптимизации с известными свойствами: точка глобального минимума, количество локальных минимумов и др.


Ниже представлены результаты работы синхронного параллельного алгоритма глобального поиска  с использованием дерева решений для нахождения областей притяжения локальных минимумов. Численное сравнение проводилось на двух классах функций GKLS (Simple и Hard из \cite{Sergeyev2006}) размерностей 2, 3, 4 и 5.
Эти два класса отличаются размером области притяжения точки глобального минимума.
Для простого класса задач радиус области притяжения в три раза больше, чем для сложного.

Критерием остановки алгоритмов было попадание очередных пробных точек в $\varepsilon$-близость к истинному глобальному минимуму. В таблицах \ref{tab:1} \ref{tab:2} сравниваются усредненные числа итераций, выполняемых алгоритмами при решении задач, и значения ускорения времени относительно последовательного прогона. Распараллеливание выполнено по технологии MPI, эксперимент выполнен на 8, 16, 32 и 64 процессах.


\begin{table}[!ht]
	\caption{Ускорение, выполненных разными алгоритмами}
	\label{tab:2}
	\centering	
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		2 & 8 mpi & 16 mpi & 32 mpi & 64 mpi  \\ \hline
		hard & 6,416962112 & 7,429653192 & 16,44977408 & 7,698208433  \\ \hline
		simple & 11,19015498 & 5,017982941 & 13,58978911 & 30,41682225  \\ \hline
		3 & ~ & ~ & ~ &   \\ \hline
		hard & 4,010618178 & 6,760486061 & 22,40788476 & 23,48393504  \\ \hline
		simple & 3,975141387 & 5,069355695 & 14,97196995 & 14,48568142  \\ \hline
		4 & ~ & ~ & ~ &   \\ \hline
		hard & 3,516700024 & 7,767619059 & 15,63797476 & 22,60877765  \\ \hline
		simple & 4,204314065 & 6,771626114 & 7,265190222 & 5,789337963  \\ \hline
		5 & ~ & ~ & ~ &   \\ \hline
		hard & 10,28406793 & 18,45856679 & 38,75628766 & 30,57030882  \\ \hline
		simple & 10,79552171 & 15,93214599 & 31,96240356 & 31,65420434  \\ \hline
	\end{tabular}
\end{table}




Как видно из таблицы \ref{tab:2}, ускорение итерации достаточно велико для задач любой размерности.





\section{Conclusion} 

На данный момент в результате проделанной работы удалось совместить алгоритм глобального поиска с методом локальной оптимизации. В отличие от известных многозаходных схем решение о запуске локального метода принимается с помощью дерева решений. Использование такой комбинации методов позволяет значительно ускорить работу алгоритма.

Параллельная версия алгоритма сохраняет свойства своего последовательного прототипа, что было подтверждено численными экспериментами по решению серии из нескольких сотен задач различной размерности. Предлагаемая схема позволяет использовать преимущества как распараллеливания, так и быстрого поиска локальных экстремумов.

Помимо использования деревьев решений для выявления областей притяжения локальных экстремумов многоэкстремальных функций, мы также планируем использовать методы машинного обучения для разделения переменных решаемой задачи. Во многих прикладных задачах оптимизации зависимость целевой функции от некоторых параметров бывает либо линейной, либо одномодальной. Выделение таких переменных в специальную группу и решение задачи с помощью параллельной схемы рекурсивной оптимизации \cite{Barkalov2020_1} позволяет сократить время решения задачи на порядки по сравнению с использованием глобального поиска сразу по всем переменным.

%
% ---- Bibliography ----
%
\bibliographystyle{spmpsci}
\bibliography{bibliography}{}











\end{document}
