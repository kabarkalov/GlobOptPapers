%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Sample document for preparing papers to  "Avtomatika i Telemekhanika"
%%  charset=windows-1251
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{a&t}
\usepackage{graphicx}

\begin{document}  %%%!!!

\year{2019}
\title{НАЗВАНИЕ СТАТЬИ ИЛИ ЗАМЕТКИ БЫВАЕТ С ФОРМУЛАМИ $a+b=c$}%
\thanks{Исследование выполнено за счет гранта Российского научного фонда (проект \mbox{№\,16-11-10150}).}

\authors{Р.Г.~СТРОНГИН, д-р~физ.-мат.~наук\\
В.П.~ГГЕРГЕЛЬ, д-р~техн.~наук\\
К.А.~БАРКАЛОВ, канд.~физ.-мат.~наук\\
(Нижегородский государственный университет им. Н.И. Лобачевского)}

\maketitle

\begin{abstract}
Краткая аннотация статьи или заметки. Иногда не бывает. Хх
хххххххххххх хххххх хххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх ххх ххххх хххххх х хххххххххххххх ххх ххххххх
хххх.
\end{abstract}


\section{Введение}

В статье рассматриваются задачи глобальной оптимизации вида
\begin{eqnarray}\label{main_problem}
& \varphi(y^\ast)=\min{\left\{\varphi(y):y\in D\right\}},\\
& D=\left\{y\in R^N: a_i\leq y_i \leq b_i, 1\leq i \leq N\right\}. \nonumber
\end{eqnarray}
Предполагается, что целевая функция может быть многоэкстремальной, задана неявно (функция вида <<черный ящик>>), а вычисление ее значений связано с решением задачи численного моделирования и является трудоемкой операцией.

Любая возможность достоверно оценить глобальный оптимум в многоэкстремальной задаче с функциями вида <<черный ящик>> принципиально основана на априорной информации, позволяющей связать возможные значения целевой функции с ее известными значениями в точках проведенных испытаний. Для многих прикладных задачах типичной является ситуация, когда ограниченное изменение вектора параметров $y$ вызывает ограниченное изменение значений $\varphi(y)$. Математической моделью, описывающей указанное предположение, является условие Липшица
\[
\left|\varphi(y')-\varphi(y'')\right|\leq L\left\|y'-y''\right\|,\; y',y'' \in D,\; 0<L<\infty.
\]
%Отметим, что значение константы $L$ обычно является априори неизвестным, что делает ее оценку одной из ключевых проблем при построении методов липшицевой оптимизации.
Предположение липшицевости целевой функции типично для многих подходов к разработке оптимизационных алгоритмов. Первые методы липшицевой оптимизации были предложены в начале 70-х годов \cite{Evtushenko71,Pijavski72,Shubert72}; с тех пор данное направление продолжает активно развиваться \cite{Jones93,Pinter96,Zilinskas08,Evtushenko07,Evtushenko09}.
Например, многие известные методы основаны на различных способах разбиения области поиска на систему подобластей и последующего выбора наиболее перспективной подобласти для размещения очередного испытания (вычисления целевой функции). Результаты в данном направлении представлены в работах \cite{Jones09,Zilinskas10,Evtushenko13,Kvasov13,Paulavicius16}.
 
В настоящее время для решения задач оптимизации с функциями вида <<черный ящик>> широко используются генетические и популяционные алгоритмы (см., например, \cite{Kureichik04,Karpenko08}), которые так или иначе основаны на идеях случайного поиска. В силу простоты реализации и использования они получили большую популярность, однако по качеству работы (численной оценкой которого может служить число корректно решенных задач из некоторого набора) они существенно уступают детерминированным алгоритмам \cite{Kvasov18,Sergeyev18}.

%В последние годы для решения задач с функциями вида ``черный ящик'' получили широкое распространение так называемые биоинспирированные алгоритмы, основанные на моделировании природных процессов и явлений. Типичными примерами методов данного класса являются генетические и популяционные алгоритмы (см., например, ). В основе указанных алгоритмов так или иначе лежит случайный поиск, т.к. они все содержат случайную компоненту (например, операции скрещивания и мутации в генетических или миграции в популяционных алгоритмах). В силу своей вероятностной природы методы данного класса обладают, вообще говоря, всюду плотной сходимостью, что невыгодно отличает их от детерминированных алгоритмов.

Одним из эффективных детерминированных методов решения задач многоэкстремальной оптимизации является \textit{информационно-статистический алгоритм глобального поиска}. Основы информационно-статистического подхода были заложены Ю.И. Неймарком \cite{Neymark66a,Neymark66b} и развиты Р.Г. Стронгиным \cite{Strongin78}. Впоследствии метод, изначально предложенный для решения безусловных задач, был успешно обобщен для решения задач с невыпуклыми ограничениями \cite{Strongin86,Strongin87} и многокритериальных задач \cite{Strongin93}.

Разработанные методы основаны на редукции исходной многомерной задачи к эквивалентной одномерной или системе одномерных подзадач и последующим решением одномерных задач эффективными методами оптимизации функций одной переменной. Предложено две такие схемы: редукция на основе кривых, заполняющих пространство (кривых Пеано, или разверток) \cite{Strongin91,Strongin00}, и схема рекурсивной вложенной оптимизации (многошаговая схема) \cite{Grishagin97,Grishagin01}. В статье \cite{Grishagin16} предложена адаптивная многошаговая схема, существенно повышающая эффективность оптимизации по сравнению с базовым прототипом. В данной работе предлагается обобщение адаптивной схемы редукции размерности, комбинирующее использование вложенной оптимизации и кривых Пеано. При таком подходе вложенные подзадачи в адаптивной схеме могут быть как одномерными, так и многомерными. В последнем случае для редукции размерности вложенных подзадач используются развертки.


%Развитие - параллельность. предложены параллельные варианты многих алгоритмов. Отметим, что в основном распараллеливают генетические алгоритмы. В статье - последние наработки в направлении алгоритма глобального поиска. 



\section{Базовый алгоритм глобального поиска}



\section{Многошаговая схема редукции размерности}

Многошаговая схема редукции размерности (схема вложненной оптимизации) основана на известном соотношении
\begin{equation}\label{nested}
\min_{y \in D}\varphi(y) = \min_{y_1\in\left[a_1,b_1\right]}\min_{y_2\in\left[a_2,b_2\right]}...\min_{y_N\in\left[a_N,b_N\right]}\varphi(y),
\end{equation}
которое позволяет свести решение исходной многомерной задачи (\ref{main_problem}) к решению семейства рекурсивно связанных одномерных позадач.

Для формального описания многошаговой схемы введем семейство функций, определяемых в соответствии с соотношениями 
\begin{equation}\label{nested_N}
\varphi_N(y_1,...,y_N) \equiv \varphi(y_1,...,y_N),
\end{equation}
\begin{equation}\label{nested_i}
\varphi_i(y_1,...,y_i) = \min_{ y_{i+1} \in\left[a_{i+1},b_{i+1}\right]} \varphi_{i+1}(y_1,...,y_i,y_{i+1}), 1\leq i\leq N-1.
\end{equation}

Тогда, в соответствии с (\ref{nested}), для решения многомерной задачи (\ref{main_problem}) достаточно решить одномерную задачу  
\begin{equation}\label{nested_1}
\varphi^* = \min_{y_1\in\left[a_1,b_1\right]}\varphi_1(y_1).
\end{equation}
Однако каждое вычисление значения функции $\varphi_1$ в некоторой фиксированной точке $y_1$ предполагает решение одномерной задачи оптимизации второго уровня 
\begin{equation}
\varphi_1(y_1) = \min_{y_2\in\left[a_2,b_2\right]}\varphi_2(y_1,y_2).
\end{equation}
Вычисление значений функции $\varphi_2$, в свою очередь, требует одномерной минимизации функции $\varphi_3$, и т.д. вплоть до
решения задачи
\begin{equation}
\varphi_{N-1}(y_1,...,y_{N-1}) = \min_{ y_{N} \in\left[a_{N},b_{N}\right]} \varphi_{N}(y_1,...,y_{N}),
\end{equation}
на последнем уровне рекурсии.

Схема вложенной оптмизации (\ref{nested_N})--(\ref{nested_1}) в предположении липшицевости одномерных функций $\overline{\varphi}_i(y_i)=\varphi_i(y_1,...,y_i)$ послужила основой для обобщения методов одномерной оптимизации [7,25,32,34,36 из JOGO] на случай многих переменных [3,12,25,26,36 из JOGO].

Дадим более подробное описание взаимосвязей между одномерными подзадачами в многошаговой схеме. Для удобства последующего изложения введем обозначение
\[
v_i = (y_1,...y_i), \; 1 \leq i\ \leq N.
\]
Тогда одномерные функции $\overline{\varphi}_i(y_i), \; 1 < i\ \leq N$, порожденные в рамках многошаговой схемы, могут быть представлены в виде
\[
\overline{\varphi_i}(y)_i = \varphi_i(v_{i-1},y_i), \; 1 < i\ \leq N,
\]
где вектор $v_{i-1}$ фиксирован. 

В соответствии с многошаговой схемой на некотором шаге поиска $i, 1 \leq i \leq k_1$  алгоритм начинает минимизировать одномерную функцию $\varphi_1(y_1)$. Для этого, используя накопленную поисковую информацию, алгоритм выбирает точку очередной итерации $\widehat{y}_1$ и вычисляет значение функции в ней. Но для того, чтобы получить значение $\varphi_1(\widehat{y}_1)$ в этой точке, должна быть решена задача минимизации функции $\varphi_2(\widehat{y}_1,y_2)$. То есть, алгоритм приостанавливает минимизацию функции $\varphi_1(y_1)$ и начинает минимизировать $\varphi_2(\widehat{y}_1,y_2)$. В процессе минимизации алгоритм генерирует последовательность точек $\{y_2^j, 1 \leq j \leq k_2\}$, для каждой из которых необходимо решить задачу минимизации функции $\varphi_3(\widehat{y}_1,\widehat{y}_2,y_3)$, и т.д. Данный процесс выполняется рекурсивно до последнего $N$-го уровня. Возникающие при этом подзадачи и взаимосвязи между ними отображены на рис. \ref{fig1}.

\begin{figure}
\begin{center}%[scale=0.3][width=0.75\textwidth]
  \includegraphics[width=0.9\textwidth]{fig1.png} 
  \caption{Взаимосвязи между подзадачами в многошаговой схеме}\label{fig1} 
\end{center}
\end{figure}


Наглядно видно, что структура взаимосвязей имеет форму дерева. Указанная структура может меняться в процессе решения многомерной задачи (\ref{main_problem}): вычисление значения функции $\varphi_i(\widehat{v}_{i-1},y_i), \; 1\leq i < N,$  на $i$-м уровне рекурсии требует решения всех подзадач в одном из поддеревьев $(i+1)$-го уровня. При этом функции $\varphi_N(\widehat{v}_{N-1},y_N)$ на $N$-м уровне являются листьями дерева задач, их значения вычисляются непосредственно.  

%недостатки многошаговой схемы

\section{Адаптивная многошаговая схема}

Рассмотрим обобщение схемы вложенной оптимизации (\ref{nested_N})--(\ref{nested_1}), позволяющее преодолеть недостатки указанные выше. Основным отличием является отказ от принципа подчитенности одномерных подзадач на основе функций $\overline{\varphi_i}(y_i), \; 1\leq i \leq N,$ порожденных в рамках многошаговой схемы. В новом подходе все возникающие подзадачи предлагается решать одновременно. Предварительное описание \textit{адаптивной многошаговой схемы} редукции размерности заключается в следующем.







Введем следующее определение.

\begin{definition}
Хххххххххххххх хххххх-хххххххх х ххххх ххххххххх ххх ххххххх хххх.
Ххххх хххххххх хххххххххххххх х ххххх.
\end{definition}

Хххх ххххххх хххххххххххххх х ххххх ххххххххх ххх ххххххх хххх.

Рассмотрим следующую задачу.

\begin{problem} \label{prob:1}
Хххххх хххххххх хххххх хххххххх х ххххх ххххххххх ххх ххххххх хххх.
Ххххх хххххххх хххххххххххххх х ххххххххх х ххххх.
\end{problem}

Хххххх хххххххх хххххх хххххххх х ххххх ххххххххх ххх ххххххх хххх.
Ххххх хххххххх хххххххххххххх х ххххххххх х ххххх.


\section{Заголовок третьего раздела}

\subsection{Заголовок подраздела}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

Сформулируем следующую теорему.

\begin{theorem}[{\cite[c.\,123]{first}}] %
Пусть выполнены следующие условия:

\begin{ruslist}
\item
первое условие;

\item
второе условие.
\end{ruslist}

Тогда справедливы следующие утверждения:

\begin{enumlist}
\item
первое утверждение;

\item
второе утверждение.
\end{enumlist}
\end{theorem}

\begin{proof}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх:
\begin{gather}
    2\times 2=4.
\end{gather}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{proof}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

\begin{corollary}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххх х хххххххххххххх ххх ххххххх хххх.
\end{corollary}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.


\subsection{Заголовок следующего подраздела}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

\begin{lemma}[(см.\ {\cite[c.\,45]{second}})] \label{lm:1}
Хххххххххххххх хххххххххххххх х хххххххххх:
\begin{multline}
    2\times 2\times 2\times 2\times 2\times 2\times 2\times 2\times
    2\times 2\times 2\times 2\times 2\times 2\times 2\times 2\times
\\
    \times 2\times 2\times 2\times 2\times 2\times 2\times 2\times 2\times
    2\times 2\times 2\times 2\times 2\times 2\times 2\times 2
\end{multline}
хххххххх ххххххх ххххххх х хххххххххххххх ххх.
\end{lemma}

Опишем следующий алгоритм.

\begin{algorithm}[(Быстрый)] \label{alg:1}
\ %%<-- этот пробел для того, чтобы первый элемент перечня был
%% на следующей строке, а не в подбор к заголовку окружения

\begin{enumlist}[.] % перечни, нумеруемые 1. 2. и т.д.
% \setcounter{enumlisti}{-1} % <-- эта команда нужна
%% для нумерации элементов перечня с нулевого
\item
Начать.

\item
Изменить.

\item
Закончить.
\end{enumlist}
\end{algorithm}

Следующая теорема утверждает сходимость алгоритма~\ref{alg:1}.

\begin{theorem}[(Теорема сходимости)] \label{th:2}
Алгоритм~{\rm\ref{alg:1}} сходится.
\end{theorem}

Можно привести замечание.

\begin{remark}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{remark}

\section{Четвертый раздел}

Данный раздел содержит несколько примеров различных окружений.

\begin{example}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

Некоторые перечни можно нумеровать русскими буквами:
\begin{ruslist} % перечни, нумеруемые а), б) и т.д.
\item
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хх
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх;

\item
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{ruslist}

А некоторые можно~"--- и латинскими буквами:

\begin{latlist} % перечни, нумеруемые а), б) и т.д.
\item
ххххххххххх ххх ххххххх хххх;

\item
хххххххххххх ххх ххххххх хххх.
\end{latlist}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{example}

Можно сформулировать утвеждение.

\begin{statement}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{statement}

Можно сформулировать предложение.

\begin{proposition}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{proposition}

\appendix{1}  % приложения можно нумеровать, если их несколько

\begin{proofoftheorem}{\ref{th:2}}
Докажем сначала, что выполнено следующее соотношение:
\begin{gather} \label{f(x)=y}
    f(x)=y.
\end{gather}
Действительно,~\dots откуда получаем, что
равенство~\eqref{f(x)=y} справедливо. Следовательно,~\dots
окончательно,~\dots

Теорема~\ref{th:2} доказана.
\end{proofoftheorem}

\begin{proofoflemma}{\ref{lm:1}}
Очевидно, что \dots Таким образом, \dots, что и требовалось
доказать.
\end{proofoflemma}


\begin{thebibliography}{10}


\bibitem{Evtushenko71}
{\it	Евтушенко Ю.Г.} Численный метод поиска глобального экстремума функций (перебор на неравномерной сетке) // Ж. вычисл. матем. и матем. физ. 1971. T. 11, №6, С. 1390--1403.

\bibitem{Pijavski72}
{\it	Пиявский С.А.} Один алгоритм отыскания абсолютного экстремума функций // Ж. вычисл. матем. и матем. физ. 1972. Т. 12, № 4. С. 888--896.

\bibitem{Shubert72}
{\it Shubert B.O.} A sequential method seeking the global maximum of a function // SIAM J. Numer. Anal. 1972. V. 9. P. 379–388.

\bibitem{Jones93} 
{\it Jones D.R., Perttunen C.D., Stuckman B.E.} Lipschitzian optimization without the Lipschitz constant // J. Optim. Theory Appl. 1993. V. 79. No. 1. P. 157--181.

\bibitem{Pinter96}
{\it Pinter J. D.} Global Optimization in Action (Continuous and Lipschitz Optimization: Algorithms, Implementations and Applications). Dordrecht: Kluwer Academic Publishers, 1996.

\bibitem{Zilinskas08}
{\it {\v Z}ilinskas J.} Branch and bound with simplicial partitions for global optimization // Math. Model. Anal. 2008. V. 13. No. 1. P. 145--159.

\bibitem{Evtushenko07}
{\it Евтушенко Ю.Г., Малкова В.У., Станевичюс А.-И. А.} Распараллеливание процесса поиска глобального экстремума // Автомат. и телемех. 2007. № 5. C. 46--58

\bibitem{Evtushenko09}
{\it Евтушенко Ю.Г., Малкова В.У., Станевичюс А.-И. А.} Параллельный поиск глобального экстремума функций многих переменных // Ж. вычисл. матем. и матем. физ. 2009. Т. 49, №2. C. 255--269.


\bibitem{Jones09}%Проверить формат ссылки!!!
{\it Jones D. R.} The direct global optimization algorithm. In: Floudas C. A., Pardalos P. M. (eds.) The Encyclopedia of Optimization, Second Edition.  Springer. 2009. P. 725--735.

\bibitem{Zilinskas10}
{\it Paulavi{\v c}ius R., {\v Z}ilinskas J., Grothey A.}: Investigation of selection strategies in branch and bound algorithm with simplicial partitions and combination of Lipschitz bounds // Optim. Lett. 2010. V. 4(1). P. 173–--83

\bibitem{Evtushenko13}
{\it Evtushenko Y. G., Posypkin M. A.} A deterministic approach to global box-constrained optimization // Optim. Lett. 2013. V. 7(4). P. 819--829

\bibitem{Kvasov13}
{\it Квасов Д.Е., Сергеев Я.Д.} Методы липшицевой глобальной оптимизации в задачах управления // Автомат. и телемех. 2013. № 9. C. 3--19.

\bibitem{Paulavicius16}
{\it Paulavi{\v c}ius R., {\v Z}ilinskas J.} Advantages of simplicial partitioning for Lipschitz optimization problems with linear constraints // Optim. Lett. 2016. V. 10(2). P. 237--246.

\bibitem{Kureichik04}
{\it Гладков Л.А., Курейчик В.В., Курейчик В.М.} Генетические алгоритмы. Учебное пособие. М.: ФИЗМАТЛИТ, 2004.

\bibitem{Karpenko08}
{\it Карпенко А.П.} Современные алгоритмы поисковой оптимизации. Алгоритмы, вдохновленные природой: учебное пособие. М.: Издательство МГТУ им. Н. Э. Баумана, 2014.

\bibitem{Kvasov18}
{\it Kvasov D.E., Mukhametzhanov M.S.} Metaheuristic vs. deterministic global optimization algorithms: The univariate case // Appl. Math. Comput. 2018. V. 318. P. 245--259.

\bibitem{Sergeyev18}
{\it Sergeyev Y., Kvasov D., Mukhametzhanov M.} On the efficiency of nature-inspired metaheuristics in expensive global optimization with limited budget // Sci.
Rep. V. 8(1). Art. No. 435.

\bibitem{Neymark66a}
{\it Неймарк Ю.И., Стронгин Р.Г.} Поиск экстремума функций по принципу максимума информации // Автомат. и телемех. 1966. № 11. С. 113--118.

\bibitem{Neymark66b}
{\it Неймарк Ю.И., Стронгин Р.Г.} Информационный подход к задаче поиска экстремума функций // Изв. АН СССР, Техническая кибернетика. 1966. № 1. С. 17--26.

\bibitem{Strongin78}
{\it Стронгин Р.Г.} Численные методы в многоэкстремальных задачах (информационно-статистические алгоритмы). М.: Наука, 1978.

\bibitem{Strongin86}
{\it Стронгин Р.Г., Маркин Д.Л.} Минимизация многоэкстремальных функций при невыпуклых ограничениях // Кибернетика. 1986. №4. С.63--69.

\bibitem{Strongin87}
{\it Маркин Д.Л., Стронгин Р.Г.} Метод решения многоэкстремальных задач с невыпуклыми ограничениями, использующий априорную информацию об оценках оптимума // Ж. вычисл. матем. и матем. физ. 1987. Т.27, № 1. С.52--61.

\bibitem{Strongin93}
{\it Маркин Д.Л., Стронгин Р.Г.} О равномерной оценке множества слабоэффективных точек в многоэкстремальных многокритериальных задачах оптимизации // Ж. вычисл. матем. и матем. физ. 1993 Т. 33, № 2. С. 195--205.

\bibitem{Strongin91}
{\it Стронгин Р.Г.} Параллельная многоэкстремальная оптимизация с использованием множества разверток // Ж. вычисл. матем. и матем. физ. 1991. T. 31, № 8. С. 1173--1185.

\bibitem{Strongin00}
{\it Strongin R.G., Sergeev Ya.D.} Global Optimization with Non-Convex Constraints. Sequential and Parallel Algorithms. Dordrecht: Kluwer Academic Publishers, 2000.

\bibitem{Strongin13}
{\it Стронгин Р.Г., Гергель В.П., Гришагин В.А., Баркалов К.А.} Параллельные вычисления в задачах глобальной оптимизации. М.: Издательство Московского университета, 2013.

\bibitem{Grishagin97}

{\it Grishagin V.A., Sergeyev Y.D., Strongin R.G.} Parallel characteristical algorithms for solving problems of global optimization // J. Glob. Optim. 1997. V. 10(2). P. 185--206.

\bibitem{Grishagin01}
{\it Sergeyev Y., Grishagin V.} Parallel asynchronous global search and the nested optimization scheme // J. Comput. Anal. Appl. 2001. V. 3(2). P. 123--145.

\bibitem{Grishagin16}
{\it Gergel V., Grishagin V., Gergel A.} Adaptive nested optimization scheme for multidimensional global search // J. Glob. Optim. 2016. V. 66(1). P. 35--51.




%\bibitem{Sergeyev00} Kvasov, D.E., Pizzuti, C., Sergeyev, Y.D.: Local tuning and partition strategies for diagonal GO methods. Numer. Math. 94(1), 93--106 (2003)
%\bibitem{Sergeyev06} Sergeyev, Ya.D., Kvasov, D.E.: Global search based on efficient diagonal partitions and a set of Lipschitz constants. SIAM J. Optim. 16(3), 910--937 (2006)

%\bibitem{third} {\it Третий Т.Т.} Публикация в трудах конференции // Тр. Ин-та такого-то РАН. 2000. Т. 1. № 2. С. 3--4.
%\bibitem{forth} {\it Четвертый Ч.Ч.} Публикация по теме в серийном издании или сборнике / Сб. научн. тр. к 20-летию Института. Новосибирск, Изд-во <<Пирогов>>, 2000. Т. 1. № 2. С. 3--4.
%\bibitem{fifth} {\it Fifth F.} Some journal publication in English // Appl. Math. Comput. J., Elsevier Publ. 1925. V. 501. No. 1. P. 1234--5678.
%\bibitem{sixth} {\it Sixth J.Th.} A book in English. Boston: Springer, 1991.

\end{thebibliography}

\AdditionalInformation{Стронгин Р.Г.}{Нижегородский государственный университет им. Н.И. Лобачевского, президент, Нижний Новгород}{strongin@unn.ru}

\AdditionalInformation{Гергель В.П.}{Нижегородский государственный университет им. Н.И. Лобачевского, директор института информационных технологий, математики и механики, Нижний Новгород}{gergel@unn.ru}

\AdditionalInformation{Баркалов К.А.}{Нижегородский государственный университет им. Н.И. Лобачевского, доцент кафедры математического обеспечения и суперкомпьютерных технологий института информационных технологий, математики и механики, Нижний Новгород}{konstantin.barkalov@itmm.unn.ru}

\end{document}
